{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense,Conv2D, BatchNormalization, Activation,Dropout\n",
    "from keras.layers import AveragePooling2D, Input, Flatten,GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import skimage\n",
    "from keras.models import Model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generate(data_path, batch_size, argumentation = True, shuffle=False):\n",
    "    while True:\n",
    "        fid = h5py.File(data_path, 'r')\n",
    "        data_len = fid['sen1'].shape[0]\n",
    "        c = [i for i in range(int(data_len/batch_size) + 1)]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(c)\n",
    "        for i in c:\n",
    "            t = np.random.rand()\n",
    "            try:\n",
    "                y_b = np.array((fid['label'][i * batch_size: (i+1)*batch_size]))\n",
    "                x_b1 = fid['sen1'][i * batch_size: (i+1)*batch_size]\n",
    "                x_b2 = fid['sen2'][i * batch_size: (i+1)*batch_size]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([1,2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)\n",
    "            except:\n",
    "                x_b1 = fid['sen1'][i*batch_size:]\n",
    "                x_b2 = fid['sen2'][i*batch_size:]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                y_b = np.array((fid['label'][i * batch_size:]))\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        \n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3] , y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_v2(input_tensor, depth, num_classes=17):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    #inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=input_tensor,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #y = Flatten()(x)\n",
    "    #outputs = Dense(num_classes,\n",
    "     #               activation='softmax',\n",
    "     #               kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    #model = Model(inputs=inputs, outputs=outputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "def focal_loss(classes_num, gamma=2., alpha=.25, e=0.1):\n",
    "    # classes_num contains sample number of each classes\n",
    "    def focal_loss_fixed(target_tensor, prediction_tensor):\n",
    "        '''\n",
    "        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes\n",
    "        target_tensor is the label tensor, same shape as predcition_tensor\n",
    "        '''\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.python.ops import array_ops\n",
    "        from keras import backend as K\n",
    "\n",
    "        #1# get focal loss with no balanced weight which presented in paper function (4)\n",
    "        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
    "        one_minus_p = array_ops.where(tf.greater(target_tensor,zeros), target_tensor - prediction_tensor, zeros)\n",
    "        FT = -1 * (one_minus_p ** gamma) * tf.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))\n",
    "\n",
    "        #2# get balanced weight alpha\n",
    "        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
    "\n",
    "        total_num = float(sum(classes_num))\n",
    "        classes_w_t1 = [ total_num / ff for ff in classes_num ]\n",
    "        sum_ = sum(classes_w_t1)\n",
    "        classes_w_t2 = [ ff/sum_ for ff in classes_w_t1 ]   #scale\n",
    "        classes_w_tensor = tf.convert_to_tensor(classes_w_t2, dtype=prediction_tensor.dtype)\n",
    "        classes_weight += classes_w_tensor\n",
    "\n",
    "        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)\n",
    "\n",
    "        #3# get balanced focal loss\n",
    "        balanced_fl = alpha * FT\n",
    "        balanced_fl = tf.reduce_sum(balanced_fl)\n",
    "\n",
    "        #4# add other op to prevent overfit\n",
    "        # reference : https://spaces.ac.cn/archives/4493\n",
    "        nb_classes = len(classes_num)\n",
    "        fianal_loss = (1-e) * balanced_fl + e * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)\n",
    "\n",
    "        return fianal_loss\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape1 = (32, 32, 8)\n",
    "input_shape2 = (32, 32, 10)\n",
    "input_shape3 = (32, 32, 18)\n",
    "input_1 = Input(shape=input_shape1)\n",
    "input_2 = Input(shape=input_shape2)\n",
    "input_3 = Input(shape=input_shape3)\n",
    "L1 = BatchNormalization()(input_1)\n",
    "L2 = BatchNormalization()(input_2)\n",
    "L3 = BatchNormalization()(input_3)\n",
    "L1 = resnet_v2(input_tensor=L1, depth=56)\n",
    "L2 = resnet_v2(input_tensor=L2, depth=56)\n",
    "L3 = resnet_v2(input_tensor=L3, depth=56)\n",
    "\n",
    "#L1 = Conv2D(16, kernel_size=3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(L1)\n",
    "#L2 = Conv2D(16, kernel_size=3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(L2)\n",
    "\n",
    "\n",
    "L = keras.layers.concatenate([L1, L2, L3])\n",
    "L = Dense(512, activation='relu')(L)\n",
    "L = BatchNormalization()(L)\n",
    "L = Dropout(0.5)(L)\n",
    "L = Dense(17, activation='softmax', kernel_initializer='he_normal')(L)\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=L)\n",
    "class_nums = [5068, 24431, 31693,  8651, 16493, 35290,  3269, 39326,\n",
    "       13584, 11954, 42902,  9514,  9165, 41377,  2392,  7898,\n",
    "       49359]\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "model.compile(loss=focal_loss(class_nums), optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = './models'\n",
    "model_name = 'LCZ_model_3_inputsss-950-loss-f-56.h5'\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc',save_best_only=True, save_weights_only=True)\n",
    "lr_reducr = ReduceLROnPlateau(factor=0.1,cooldown=0, patience=3, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor='val_acc',patience=5)\n",
    "calls = [checkpoint, lr_reducr, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = data_generate('./training.h5', batch_size=128, argumentation=True, shuffle=True)\n",
    "val_gen = data_generate('./validation.h5', batch_size=128, argumentation=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2753/2753 [==============================] - 1627s 591ms/step - loss: 3.8941 - acc: 0.6983 - val_loss: 5.2324 - val_acc: 0.5471\n",
      "Epoch 2/200\n",
      "2753/2753 [==============================] - 1597s 580ms/step - loss: 2.2280 - acc: 0.7791 - val_loss: 4.8957 - val_acc: 0.5273\n",
      "Epoch 3/200\n",
      "2753/2753 [==============================] - 1576s 572ms/step - loss: 1.8555 - acc: 0.8062 - val_loss: 5.3669 - val_acc: 0.5602\n",
      "Epoch 4/200\n",
      "2753/2753 [==============================] - 1573s 571ms/step - loss: 1.6736 - acc: 0.8244 - val_loss: 5.6762 - val_acc: 0.5704\n",
      "Epoch 5/200\n",
      "2753/2753 [==============================] - 1565s 568ms/step - loss: 1.5526 - acc: 0.8365 - val_loss: 6.2178 - val_acc: 0.5735\n",
      "Epoch 6/200\n",
      "2753/2753 [==============================] - 1585s 576ms/step - loss: 1.4697 - acc: 0.8469 - val_loss: 5.4164 - val_acc: 0.5348\n",
      "Epoch 7/200\n",
      "2753/2753 [==============================] - 1565s 569ms/step - loss: 1.2409 - acc: 0.8833 - val_loss: 5.2513 - val_acc: 0.6136\n",
      "Epoch 8/200\n",
      "2753/2753 [==============================] - 1557s 566ms/step - loss: 1.1776 - acc: 0.8933 - val_loss: 5.3942 - val_acc: 0.5960\n",
      "Epoch 9/200\n",
      "2753/2753 [==============================] - 1572s 571ms/step - loss: 1.1408 - acc: 0.8980 - val_loss: 5.5318 - val_acc: 0.5997\n",
      "Epoch 10/200\n",
      "2753/2753 [==============================] - 1568s 569ms/step - loss: 1.1011 - acc: 0.9046 - val_loss: 5.3893 - val_acc: 0.6019\n",
      "Epoch 11/200\n",
      "2753/2753 [==============================] - 1565s 568ms/step - loss: 1.0960 - acc: 0.9056 - val_loss: 5.3958 - val_acc: 0.6084\n",
      "Epoch 12/200\n",
      "2753/2753 [==============================] - 1564s 568ms/step - loss: 1.0923 - acc: 0.9062 - val_loss: 5.4433 - val_acc: 0.6072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6184b0e160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, steps_per_epoch=352366//128+1, epochs=200,  verbose=1,callbacks=calls, validation_data=val_gen,\n",
    "                   validation_steps=24119//128+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generate2(data1, data2, data3, batch_size, argumentation = True, shuffle=False):\n",
    "    while True:\n",
    "        data_len = data1.shape[0]\n",
    "        c = [i for i in range(int(data_len/batch_size) + 1)]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(c)\n",
    "        for i in c:\n",
    "            t = np.random.rand()\n",
    "            try:\n",
    "                y_b = data3[i*batch_size:(i+1)*batch_size]\n",
    "                x_b1 = data1[i * batch_size: (i+1)*batch_size]\n",
    "                x_b2 = data2[i * batch_size: (i+1)*batch_size]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)\n",
    "            except:\n",
    "                y_b = data3[i*batch_size:]\n",
    "                x_b1 = data1[i*batch_size:]\n",
    "                x_b2 = data2[i*batch_size:]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid = h5py.File('./validation.h5', 'r')\n",
    "valid_s1 = valid['sen1']\n",
    "valid_s2 = valid['sen2']\n",
    "valid_label = valid['label']\n",
    "valid_s1 = np.array(valid_s1)\n",
    "valid_s2 = np.array(valid_s2)\n",
    "valid_label = np.array(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24119, 32, 32, 8), (24119, 32, 32, 10), (24119, 17))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_s1.shape,valid_s2.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018)\n",
    "kf = KFold(n_splits=5, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.argmax(valid_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = h5py.File('./round2_test_a_20190121.h5')\n",
    "test_s1 = np.array(test['sen1'])\n",
    "test_s2 = np.array(test['sen2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19295.2, 4823.8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24119*0.8,24119*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 88s 585ms/step - loss: 2.8139 - acc: 0.7126 - val_loss: 1.9973 - val_acc: 0.7814\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.9700 - acc: 0.7893 - val_loss: 1.6640 - val_acc: 0.8406\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 77s 512ms/step - loss: 1.7640 - acc: 0.8150 - val_loss: 1.5068 - val_acc: 0.8532\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 77s 510ms/step - loss: 1.6279 - acc: 0.8339 - val_loss: 1.4362 - val_acc: 0.8586\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 77s 510ms/step - loss: 1.5339 - acc: 0.8435 - val_loss: 1.3562 - val_acc: 0.8719\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.4586 - acc: 0.8551 - val_loss: 1.3207 - val_acc: 0.8884\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.4074 - acc: 0.8627 - val_loss: 1.2627 - val_acc: 0.8919\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 77s 512ms/step - loss: 1.3486 - acc: 0.8745 - val_loss: 1.2341 - val_acc: 0.8975\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.3109 - acc: 0.8788 - val_loss: 1.2192 - val_acc: 0.9056\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.2682 - acc: 0.8877 - val_loss: 1.1761 - val_acc: 0.9083\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.2353 - acc: 0.8915 - val_loss: 1.1960 - val_acc: 0.9147\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 1.2155 - acc: 0.8947 - val_loss: 1.1601 - val_acc: 0.9135\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 77s 512ms/step - loss: 1.1830 - acc: 0.9002 - val_loss: 1.1661 - val_acc: 0.9151\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 77s 510ms/step - loss: 1.1568 - acc: 0.9059 - val_loss: 1.1353 - val_acc: 0.9189\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.1453 - acc: 0.9077 - val_loss: 1.1212 - val_acc: 0.9205\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.1206 - acc: 0.9127 - val_loss: 1.1207 - val_acc: 0.9207\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 77s 512ms/step - loss: 1.0974 - acc: 0.9170 - val_loss: 1.1137 - val_acc: 0.9247\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 76s 503ms/step - loss: 1.0822 - acc: 0.9196 - val_loss: 1.1084 - val_acc: 0.9236\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 77s 513ms/step - loss: 1.0765 - acc: 0.9212 - val_loss: 1.0926 - val_acc: 0.9269\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 77s 512ms/step - loss: 1.0647 - acc: 0.9229 - val_loss: 1.0827 - val_acc: 0.9321\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 77s 512ms/step - loss: 1.0444 - acc: 0.9266 - val_loss: 1.0738 - val_acc: 0.9331\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 1.0253 - acc: 0.9300 - val_loss: 1.0510 - val_acc: 0.9327\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 77s 510ms/step - loss: 1.0210 - acc: 0.9326 - val_loss: 1.0539 - val_acc: 0.9336\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.0100 - acc: 0.9357 - val_loss: 1.0442 - val_acc: 0.9394\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.9896 - acc: 0.9381 - val_loss: 1.0592 - val_acc: 0.9375\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 0.9905 - acc: 0.9379 - val_loss: 1.0354 - val_acc: 0.9414\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 76s 503ms/step - loss: 0.9770 - acc: 0.9413 - val_loss: 1.0431 - val_acc: 0.9348\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.9686 - acc: 0.9428 - val_loss: 1.0624 - val_acc: 0.9410\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 0.9608 - acc: 0.9449 - val_loss: 1.0330 - val_acc: 0.9476\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.9377 - acc: 0.9487 - val_loss: 1.0235 - val_acc: 0.9447\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.9385 - acc: 0.9485 - val_loss: 1.0049 - val_acc: 0.9456\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.9296 - acc: 0.9494 - val_loss: 1.0452 - val_acc: 0.9381\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.9177 - acc: 0.9522 - val_loss: 1.0253 - val_acc: 0.9433\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 0.9135 - acc: 0.9533 - val_loss: 0.9957 - val_acc: 0.9503\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 76s 503ms/step - loss: 0.9063 - acc: 0.9552 - val_loss: 1.0089 - val_acc: 0.9443\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8969 - acc: 0.9577 - val_loss: 1.0091 - val_acc: 0.9497\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 76s 503ms/step - loss: 0.8949 - acc: 0.9573 - val_loss: 1.0278 - val_acc: 0.9491\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 79s 523ms/step - loss: 0.8903 - acc: 0.9584 - val_loss: 1.0055 - val_acc: 0.9540\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 77s 512ms/step - loss: 0.8745 - acc: 0.9626 - val_loss: 0.9920 - val_acc: 0.9569\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8578 - acc: 0.9649 - val_loss: 0.9678 - val_acc: 0.9565\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 77s 512ms/step - loss: 0.8649 - acc: 0.9648 - val_loss: 0.9774 - val_acc: 0.9584\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8541 - acc: 0.9678 - val_loss: 0.9748 - val_acc: 0.9549\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8558 - acc: 0.9679 - val_loss: 0.9994 - val_acc: 0.9545\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8531 - acc: 0.9683 - val_loss: 0.9899 - val_acc: 0.9540\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 0.8517 - acc: 0.9679 - val_loss: 0.9586 - val_acc: 0.9594\n",
      "Epoch 46/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 0.8569 - acc: 0.9663 - val_loss: 0.9616 - val_acc: 0.9598\n",
      "Epoch 47/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8482 - acc: 0.9685 - val_loss: 0.9897 - val_acc: 0.9551\n",
      "Epoch 48/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8535 - acc: 0.9697 - val_loss: 0.9688 - val_acc: 0.9569\n",
      "Epoch 49/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8541 - acc: 0.9682 - val_loss: 0.9704 - val_acc: 0.9569\n",
      "Epoch 50/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8534 - acc: 0.9692 - val_loss: 0.9700 - val_acc: 0.9578\n",
      "Epoch 51/200\n",
      "151/151 [==============================] - 76s 504ms/step - loss: 0.8475 - acc: 0.9704 - val_loss: 0.9842 - val_acc: 0.9547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f6184fd8b48>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f6184fd8ba0>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 89s 589ms/step - loss: 2.8173 - acc: 0.7134 - val_loss: 2.0302 - val_acc: 0.7679\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 76s 507ms/step - loss: 2.0010 - acc: 0.7877 - val_loss: 1.6723 - val_acc: 0.8190\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 1.7964 - acc: 0.8107 - val_loss: 1.5501 - val_acc: 0.8441\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 1.6465 - acc: 0.8293 - val_loss: 1.4121 - val_acc: 0.8542\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 1.5324 - acc: 0.8411 - val_loss: 1.3814 - val_acc: 0.8602\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 77s 508ms/step - loss: 1.4710 - acc: 0.8509 - val_loss: 1.3136 - val_acc: 0.8774\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 1.4106 - acc: 0.8635 - val_loss: 1.2859 - val_acc: 0.8803\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 1.3682 - acc: 0.8687 - val_loss: 1.2712 - val_acc: 0.8892\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 77s 508ms/step - loss: 1.3202 - acc: 0.8775 - val_loss: 1.2257 - val_acc: 0.8963\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 1.2954 - acc: 0.8827 - val_loss: 1.1936 - val_acc: 0.8998\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 1.2587 - acc: 0.8855 - val_loss: 1.1770 - val_acc: 0.9029\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 1.2239 - acc: 0.8914 - val_loss: 1.1624 - val_acc: 0.9074\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 1.1995 - acc: 0.8973 - val_loss: 1.1419 - val_acc: 0.9120\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 1.1704 - acc: 0.9025 - val_loss: 1.1489 - val_acc: 0.9132\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 77s 508ms/step - loss: 1.1589 - acc: 0.9083 - val_loss: 1.1257 - val_acc: 0.9197\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 75s 498ms/step - loss: 1.1355 - acc: 0.9092 - val_loss: 1.1219 - val_acc: 0.9182\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 1.1146 - acc: 0.9130 - val_loss: 1.0774 - val_acc: 0.9271\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 1.1004 - acc: 0.9143 - val_loss: 1.0887 - val_acc: 0.9246\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 1.0805 - acc: 0.9198 - val_loss: 1.0793 - val_acc: 0.9265\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 1.0782 - acc: 0.9181 - val_loss: 1.0660 - val_acc: 0.9238\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 1.0491 - acc: 0.9246 - val_loss: 1.0520 - val_acc: 0.9290\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 1.0386 - acc: 0.9296 - val_loss: 1.0423 - val_acc: 0.9306\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 75s 498ms/step - loss: 1.0429 - acc: 0.9267 - val_loss: 1.0352 - val_acc: 0.9244\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 75s 498ms/step - loss: 1.0188 - acc: 0.9324 - val_loss: 1.0247 - val_acc: 0.9300\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.9984 - acc: 0.9359 - val_loss: 1.0403 - val_acc: 0.9346\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 75s 498ms/step - loss: 0.9851 - acc: 0.9401 - val_loss: 1.0247 - val_acc: 0.9339\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 0.9719 - acc: 0.9411 - val_loss: 1.0154 - val_acc: 0.9354\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.9726 - acc: 0.9411 - val_loss: 1.0045 - val_acc: 0.9408\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 75s 498ms/step - loss: 0.9613 - acc: 0.9450 - val_loss: 0.9925 - val_acc: 0.9373\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.9620 - acc: 0.9437 - val_loss: 0.9936 - val_acc: 0.9414\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.9441 - acc: 0.9476 - val_loss: 1.0017 - val_acc: 0.9439\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 75s 498ms/step - loss: 0.9369 - acc: 0.9473 - val_loss: 1.0041 - val_acc: 0.9373\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.9335 - acc: 0.9499 - val_loss: 1.0063 - val_acc: 0.9422\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 77s 508ms/step - loss: 0.9085 - acc: 0.9559 - val_loss: 0.9665 - val_acc: 0.9480\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 75s 498ms/step - loss: 0.9043 - acc: 0.9554 - val_loss: 0.9680 - val_acc: 0.9472\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.8984 - acc: 0.9575 - val_loss: 0.9591 - val_acc: 0.9501\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 0.9010 - acc: 0.9579 - val_loss: 0.9646 - val_acc: 0.9472\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 0.9075 - acc: 0.9560 - val_loss: 0.9581 - val_acc: 0.9478\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 0.8920 - acc: 0.9577 - val_loss: 0.9572 - val_acc: 0.9470\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.8929 - acc: 0.9585 - val_loss: 0.9556 - val_acc: 0.9520\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 0.8929 - acc: 0.9595 - val_loss: 0.9442 - val_acc: 0.9513\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 0.8972 - acc: 0.9579 - val_loss: 0.9685 - val_acc: 0.9501\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 75s 498ms/step - loss: 0.8872 - acc: 0.9603 - val_loss: 0.9608 - val_acc: 0.9509\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 0.8946 - acc: 0.9590 - val_loss: 0.9557 - val_acc: 0.9497\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 75s 498ms/step - loss: 0.8920 - acc: 0.9594 - val_loss: 0.9455 - val_acc: 0.9503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f60d05937d8>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f6184fd8b48>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 93s 615ms/step - loss: 2.8511 - acc: 0.7096 - val_loss: 2.0189 - val_acc: 0.7688\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 2.0072 - acc: 0.7830 - val_loss: 1.7125 - val_acc: 0.8287\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.7777 - acc: 0.8091 - val_loss: 1.5745 - val_acc: 0.8459\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 77s 513ms/step - loss: 1.6245 - acc: 0.8294 - val_loss: 1.4806 - val_acc: 0.8608\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.5369 - acc: 0.8408 - val_loss: 1.4235 - val_acc: 0.8669\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 78s 513ms/step - loss: 1.4481 - acc: 0.8519 - val_loss: 1.3754 - val_acc: 0.8785\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 77s 513ms/step - loss: 1.4082 - acc: 0.8603 - val_loss: 1.3369 - val_acc: 0.8855\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 78s 515ms/step - loss: 1.3482 - acc: 0.8693 - val_loss: 1.2966 - val_acc: 0.8938\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.3095 - acc: 0.8775 - val_loss: 1.2900 - val_acc: 0.8949\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 77s 513ms/step - loss: 1.2692 - acc: 0.8839 - val_loss: 1.2666 - val_acc: 0.9023\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 1.2359 - acc: 0.8919 - val_loss: 1.2663 - val_acc: 0.8893\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 77s 513ms/step - loss: 1.2069 - acc: 0.8945 - val_loss: 1.2353 - val_acc: 0.9075\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.1883 - acc: 0.8994 - val_loss: 1.1990 - val_acc: 0.9090\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.1667 - acc: 0.9040 - val_loss: 1.1916 - val_acc: 0.9121\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 1.1403 - acc: 0.9079 - val_loss: 1.1840 - val_acc: 0.9100\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.1217 - acc: 0.9109 - val_loss: 1.1776 - val_acc: 0.9141\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 77s 513ms/step - loss: 1.1084 - acc: 0.9146 - val_loss: 1.1462 - val_acc: 0.9154\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.0827 - acc: 0.9183 - val_loss: 1.1430 - val_acc: 0.9206\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 1.0683 - acc: 0.9216 - val_loss: 1.1707 - val_acc: 0.9133\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 78s 515ms/step - loss: 1.0512 - acc: 0.9245 - val_loss: 1.1260 - val_acc: 0.9231\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.0462 - acc: 0.9263 - val_loss: 1.1294 - val_acc: 0.9258\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.0321 - acc: 0.9266 - val_loss: 1.0891 - val_acc: 0.9289\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 78s 513ms/step - loss: 1.0180 - acc: 0.9328 - val_loss: 1.0995 - val_acc: 0.9330\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 1.0155 - acc: 0.9334 - val_loss: 1.1066 - val_acc: 0.9270\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 1.0012 - acc: 0.9367 - val_loss: 1.1003 - val_acc: 0.9293\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 0.9920 - acc: 0.9376 - val_loss: 1.0822 - val_acc: 0.9359\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.9864 - acc: 0.9381 - val_loss: 1.1274 - val_acc: 0.9307\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.9721 - acc: 0.9404 - val_loss: 1.0800 - val_acc: 0.9351\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 78s 515ms/step - loss: 0.9480 - acc: 0.9461 - val_loss: 1.0680 - val_acc: 0.9380\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.9574 - acc: 0.9436 - val_loss: 1.1023 - val_acc: 0.9376\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.9401 - acc: 0.9472 - val_loss: 1.0662 - val_acc: 0.9374\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 78s 515ms/step - loss: 0.9282 - acc: 0.9498 - val_loss: 1.0980 - val_acc: 0.9434\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.9240 - acc: 0.9523 - val_loss: 1.0821 - val_acc: 0.9341\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 0.9072 - acc: 0.9552 - val_loss: 1.0797 - val_acc: 0.9370\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 0.9087 - acc: 0.9555 - val_loss: 1.0455 - val_acc: 0.9432\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.9057 - acc: 0.9552 - val_loss: 1.0715 - val_acc: 0.9374\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 76s 505ms/step - loss: 0.8973 - acc: 0.9577 - val_loss: 1.0696 - val_acc: 0.9430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f6055805e60>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f60d05937d8>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 94s 621ms/step - loss: 2.8342 - acc: 0.7062 - val_loss: 2.0013 - val_acc: 0.7787\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 77s 510ms/step - loss: 1.9920 - acc: 0.7856 - val_loss: 1.6821 - val_acc: 0.8293\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 1.7679 - acc: 0.8106 - val_loss: 1.5354 - val_acc: 0.8451\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 1.6401 - acc: 0.8301 - val_loss: 1.4492 - val_acc: 0.8608\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 77s 510ms/step - loss: 1.5407 - acc: 0.8437 - val_loss: 1.3842 - val_acc: 0.8714\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 77s 510ms/step - loss: 1.4643 - acc: 0.8513 - val_loss: 1.3346 - val_acc: 0.8782\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 1.4176 - acc: 0.8606 - val_loss: 1.2748 - val_acc: 0.8917\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 1.3651 - acc: 0.8684 - val_loss: 1.2563 - val_acc: 0.8921\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 1.3370 - acc: 0.8736 - val_loss: 1.2344 - val_acc: 0.8946\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.2726 - acc: 0.8800 - val_loss: 1.2018 - val_acc: 0.9077\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 1.2512 - acc: 0.8877 - val_loss: 1.1900 - val_acc: 0.9106\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.2161 - acc: 0.8929 - val_loss: 1.1547 - val_acc: 0.9172\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 79s 524ms/step - loss: 1.1997 - acc: 0.8954 - val_loss: 1.1264 - val_acc: 0.9179\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 1.1755 - acc: 0.9016 - val_loss: 1.1479 - val_acc: 0.9160\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 77s 508ms/step - loss: 1.1475 - acc: 0.9055 - val_loss: 1.1246 - val_acc: 0.9214\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 1.1300 - acc: 0.9072 - val_loss: 1.0973 - val_acc: 0.9272\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 76s 500ms/step - loss: 1.1195 - acc: 0.9109 - val_loss: 1.0879 - val_acc: 0.9251\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 75s 499ms/step - loss: 1.0883 - acc: 0.9165 - val_loss: 1.0718 - val_acc: 0.9270\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 77s 512ms/step - loss: 1.0781 - acc: 0.9191 - val_loss: 1.0955 - val_acc: 0.9303\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 1.0589 - acc: 0.9241 - val_loss: 1.0724 - val_acc: 0.9286\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 77s 511ms/step - loss: 1.0412 - acc: 0.9267 - val_loss: 1.0776 - val_acc: 0.9349\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 1.0312 - acc: 0.9306 - val_loss: 1.0486 - val_acc: 0.9347\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 78s 514ms/step - loss: 1.0221 - acc: 0.9308 - val_loss: 1.0470 - val_acc: 0.9425\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 1.0148 - acc: 0.9321 - val_loss: 1.0214 - val_acc: 0.9398\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 1.0129 - acc: 0.9338 - val_loss: 1.0269 - val_acc: 0.9332\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 77s 510ms/step - loss: 0.9938 - acc: 0.9352 - val_loss: 1.0148 - val_acc: 0.9432\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 0.9788 - acc: 0.9398 - val_loss: 1.0153 - val_acc: 0.9423\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 0.9665 - acc: 0.9432 - val_loss: 1.0393 - val_acc: 0.9376\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 0.9593 - acc: 0.9434 - val_loss: 0.9935 - val_acc: 0.9438\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 77s 509ms/step - loss: 0.9496 - acc: 0.9452 - val_loss: 0.9877 - val_acc: 0.9492\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 0.9514 - acc: 0.9458 - val_loss: 0.9866 - val_acc: 0.9465\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 0.9322 - acc: 0.9482 - val_loss: 1.0084 - val_acc: 0.9452\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 0.9206 - acc: 0.9515 - val_loss: 0.9980 - val_acc: 0.9463\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 0.9227 - acc: 0.9526 - val_loss: 0.9971 - val_acc: 0.9454\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 76s 501ms/step - loss: 0.9080 - acc: 0.9545 - val_loss: 0.9674 - val_acc: 0.9465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f5f604a7990>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f6055805e60>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 2.8071 - acc: 0.7077 - val_loss: 1.9850 - val_acc: 0.7735\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 1.9727 - acc: 0.7873 - val_loss: 1.6937 - val_acc: 0.8293\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 78s 516ms/step - loss: 1.7436 - acc: 0.8130 - val_loss: 1.5880 - val_acc: 0.8432\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 1.6138 - acc: 0.8278 - val_loss: 1.4673 - val_acc: 0.8590\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 78s 518ms/step - loss: 1.5260 - acc: 0.8442 - val_loss: 1.3957 - val_acc: 0.8717\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 1.4584 - acc: 0.8542 - val_loss: 1.3607 - val_acc: 0.8767\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 78s 516ms/step - loss: 1.3925 - acc: 0.8644 - val_loss: 1.3582 - val_acc: 0.8843\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 78s 516ms/step - loss: 1.3448 - acc: 0.8716 - val_loss: 1.3184 - val_acc: 0.8866\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 78s 516ms/step - loss: 1.3174 - acc: 0.8754 - val_loss: 1.2754 - val_acc: 0.8949\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 78s 516ms/step - loss: 1.2632 - acc: 0.8846 - val_loss: 1.2591 - val_acc: 0.8951\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 78s 516ms/step - loss: 1.2346 - acc: 0.8886 - val_loss: 1.2207 - val_acc: 0.9022\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 78s 518ms/step - loss: 1.2044 - acc: 0.8947 - val_loss: 1.2147 - val_acc: 0.9072\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 1.1880 - acc: 0.8979 - val_loss: 1.1809 - val_acc: 0.9057\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 1.1636 - acc: 0.9020 - val_loss: 1.2089 - val_acc: 0.9113\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 78s 516ms/step - loss: 1.1337 - acc: 0.9068 - val_loss: 1.1626 - val_acc: 0.9122\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 78s 516ms/step - loss: 1.1193 - acc: 0.9098 - val_loss: 1.1405 - val_acc: 0.9196\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 76s 507ms/step - loss: 1.1020 - acc: 0.9142 - val_loss: 1.1296 - val_acc: 0.9194\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 78s 516ms/step - loss: 1.0930 - acc: 0.9168 - val_loss: 1.1172 - val_acc: 0.9242\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 1.0622 - acc: 0.9216 - val_loss: 1.1290 - val_acc: 0.9194\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 1.0494 - acc: 0.9234 - val_loss: 1.1321 - val_acc: 0.9232\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 1.0356 - acc: 0.9266 - val_loss: 1.1119 - val_acc: 0.9205\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 78s 519ms/step - loss: 1.0196 - acc: 0.9287 - val_loss: 1.0892 - val_acc: 0.9309\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 1.0075 - acc: 0.9323 - val_loss: 1.1222 - val_acc: 0.9269\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.9964 - acc: 0.9366 - val_loss: 1.0968 - val_acc: 0.9286\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.9891 - acc: 0.9350 - val_loss: 1.0890 - val_acc: 0.9356\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.9753 - acc: 0.9389 - val_loss: 1.0770 - val_acc: 0.9363\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.9686 - acc: 0.9402 - val_loss: 1.0758 - val_acc: 0.9392\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.9560 - acc: 0.9420 - val_loss: 1.0497 - val_acc: 0.9441\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.9544 - acc: 0.9435 - val_loss: 1.0517 - val_acc: 0.9390\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.9401 - acc: 0.9482 - val_loss: 1.0638 - val_acc: 0.9435\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.9297 - acc: 0.9500 - val_loss: 1.0367 - val_acc: 0.9435\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.9252 - acc: 0.9523 - val_loss: 1.0595 - val_acc: 0.9367\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.9136 - acc: 0.9529 - val_loss: 1.0440 - val_acc: 0.9452\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.9143 - acc: 0.9536 - val_loss: 1.0236 - val_acc: 0.9435\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 78s 519ms/step - loss: 0.9026 - acc: 0.9553 - val_loss: 1.0165 - val_acc: 0.9483\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.8973 - acc: 0.9560 - val_loss: 1.0131 - val_acc: 0.9487\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8936 - acc: 0.9579 - val_loss: 1.0666 - val_acc: 0.9487\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8878 - acc: 0.9585 - val_loss: 1.0391 - val_acc: 0.9437\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 76s 506ms/step - loss: 0.8680 - acc: 0.9643 - val_loss: 1.0280 - val_acc: 0.9481\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.8621 - acc: 0.9650 - val_loss: 0.9925 - val_acc: 0.9508\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8707 - acc: 0.9631 - val_loss: 1.0295 - val_acc: 0.9439\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 76s 507ms/step - loss: 0.8623 - acc: 0.9653 - val_loss: 1.0263 - val_acc: 0.9495\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.8502 - acc: 0.9657 - val_loss: 0.9873 - val_acc: 0.9522\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8435 - acc: 0.9688 - val_loss: 1.0229 - val_acc: 0.9514\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.8428 - acc: 0.9689 - val_loss: 1.0088 - val_acc: 0.9527\n",
      "Epoch 46/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8439 - acc: 0.9708 - val_loss: 0.9877 - val_acc: 0.9468\n",
      "Epoch 47/200\n",
      "151/151 [==============================] - 82s 541ms/step - loss: 0.8407 - acc: 0.9707 - val_loss: 1.0271 - val_acc: 0.9516\n",
      "Epoch 48/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.8223 - acc: 0.9729 - val_loss: 0.9746 - val_acc: 0.9562\n",
      "Epoch 49/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.8155 - acc: 0.9759 - val_loss: 0.9776 - val_acc: 0.9564\n",
      "Epoch 50/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.8131 - acc: 0.9762 - val_loss: 0.9588 - val_acc: 0.9585\n",
      "Epoch 51/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8124 - acc: 0.9772 - val_loss: 0.9785 - val_acc: 0.9545\n",
      "Epoch 52/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8051 - acc: 0.9793 - val_loss: 0.9780 - val_acc: 0.9562\n",
      "Epoch 53/200\n",
      "151/151 [==============================] - 78s 517ms/step - loss: 0.8087 - acc: 0.9786 - val_loss: 0.9560 - val_acc: 0.9593\n",
      "Epoch 54/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8105 - acc: 0.9765 - val_loss: 0.9779 - val_acc: 0.9564\n",
      "Epoch 55/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8073 - acc: 0.9764 - val_loss: 0.9730 - val_acc: 0.9543\n",
      "Epoch 56/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8064 - acc: 0.9784 - val_loss: 0.9580 - val_acc: 0.9587\n",
      "Epoch 57/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8023 - acc: 0.9800 - val_loss: 0.9537 - val_acc: 0.9587\n",
      "Epoch 58/200\n",
      "151/151 [==============================] - 77s 507ms/step - loss: 0.8057 - acc: 0.9792 - val_loss: 0.9623 - val_acc: 0.9589\n"
     ]
    }
   ],
   "source": [
    "pred = np.zeros((test_s1.shape[0], 17))\n",
    "test_s3 = np.concatenate([test_s1, test_s2], axis=-1)\n",
    "for i ,(tr_ind, val_ind) in enumerate(skf.split(valid_s1, y)):\n",
    "    tr1, tr2, trl = valid_s1[tr_ind], valid_s2[tr_ind], valid_label[tr_ind]\n",
    "    va1, va2, val = valid_s1[val_ind], valid_s2[val_ind], valid_label[val_ind]\n",
    "    save_dir = './model_2/'\n",
    "    model_name = 'LCZ_model_FOLD_3_input_%s_loss-f-56.h5' % str(i)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc',save_best_only=True, save_weights_only=True)\n",
    "    lr_reducr = ReduceLROnPlateau(factor=0.1,cooldown=0, patience=3, min_lr=1e-6)\n",
    "    early = EarlyStopping(monitor='val_acc',patience=5,)\n",
    "    calls = [checkpoint, lr_reducr, early]\n",
    "    class_num = [ 256., 1254., 2353.,  849.,  757., 1906.,  474., 3395., 1914.,\n",
    "        860., 2287.,  382., 1202., 2747.,  202.,  672., 2609.]\n",
    "    class_num = [int(x*0.8) for x in class_num]\n",
    "    model.compile(loss=focal_loss(class_nums), optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "    model.load_weights('./models/LCZ_model_3_inputsss-950-loss-f-56.h5')\n",
    "    tr_gen = data_generate2(tr1, tr2, trl,argumentation=True,batch_size=128,shuffle=True)\n",
    "    va_gen = data_generate2(va1, va2, val,argumentation=True,batch_size=128,shuffle=False)\n",
    "    model.fit_generator(tr_gen, steps_per_epoch=19295//128+1, epochs=200, validation_data=va_gen,validation_steps=4823//128+1, callbacks=calls)\n",
    "    model.load_weights(filepath)\n",
    "    for j in range(4):\n",
    "        te1 = np.rot90(test_s1,k=j,axes=(1,2))\n",
    "        te2 = np.rot90(test_s2,k=j,axes=(1,2))\n",
    "        te3 = np.rot90(test_s3,k=j,axes=(1,2))\n",
    "        pred += model.predict([te1, te2, te3])\n",
    "    te1 = test_s1[:,::-1,:,:]\n",
    "    te2 = test_s2[:,::-1,:,:]\n",
    "    te3 = test_s3[:,::-1,:,:]\n",
    "    pred += model.predict([te1, te2, te3])\n",
    "    te1 = test_s1[:,:,::-1,:]\n",
    "    te2 = test_s2[:,:,::-1,:]\n",
    "    te3 = test_s3[:,:,::-1,:]\n",
    "    pred += model.predict([te1, te2, te3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4842, 17)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pred / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000001, 0.99999999, 1.        , ..., 0.99999999, 1.        ,\n",
       "       1.00000001])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.zeros((4842, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i ,ind in enumerate(np.argmax(preds,axis=1)):\n",
    "    result[i][ind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.DataFrame(result,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.to_csv('round2_56-result.csv', header=None, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
