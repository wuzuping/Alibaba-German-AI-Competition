{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense,Conv2D, BatchNormalization, Activation,Dropout\n",
    "from keras.layers import AveragePooling2D, Input, Flatten,GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import skimage\n",
    "from keras.models import Model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generate(data_path, batch_size, argumentation = True, shuffle=False):\n",
    "    while True:\n",
    "        fid = h5py.File(data_path, 'r')\n",
    "        data_len = fid['sen1'].shape[0]\n",
    "        c = [i for i in range(int(data_len/batch_size) + 1)]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(c)\n",
    "        for i in c:\n",
    "            t = np.random.rand()\n",
    "            try:\n",
    "                y_b = np.array((fid['label'][i * batch_size: (i+1)*batch_size]))\n",
    "                x_b1 = fid['sen1'][i * batch_size: (i+1)*batch_size]\n",
    "                x_b2 = fid['sen2'][i * batch_size: (i+1)*batch_size]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([1,2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)\n",
    "            except:\n",
    "                x_b1 = fid['sen1'][i*batch_size:]\n",
    "                x_b2 = fid['sen2'][i*batch_size:]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                y_b = np.array((fid['label'][i * batch_size:]))\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        \n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3] , y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_v2(input_tensor, depth, num_classes=17):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    #inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=input_tensor,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #y = Flatten()(x)\n",
    "    #outputs = Dense(num_classes,\n",
    "     #               activation='softmax',\n",
    "     #               kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    #model = Model(inputs=inputs, outputs=outputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "def focal_loss(classes_num, gamma=2., alpha=.25, e=0.1):\n",
    "    # classes_num contains sample number of each classes\n",
    "    def focal_loss_fixed(target_tensor, prediction_tensor):\n",
    "        '''\n",
    "        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes\n",
    "        target_tensor is the label tensor, same shape as predcition_tensor\n",
    "        '''\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.python.ops import array_ops\n",
    "        from keras import backend as K\n",
    "\n",
    "        #1# get focal loss with no balanced weight which presented in paper function (4)\n",
    "        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
    "        one_minus_p = array_ops.where(tf.greater(target_tensor,zeros), target_tensor - prediction_tensor, zeros)\n",
    "        FT = -1 * (one_minus_p ** gamma) * tf.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))\n",
    "\n",
    "        #2# get balanced weight alpha\n",
    "        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
    "\n",
    "        total_num = float(sum(classes_num))\n",
    "        classes_w_t1 = [ total_num / ff for ff in classes_num ]\n",
    "        sum_ = sum(classes_w_t1)\n",
    "        classes_w_t2 = [ ff/sum_ for ff in classes_w_t1 ]   #scale\n",
    "        classes_w_tensor = tf.convert_to_tensor(classes_w_t2, dtype=prediction_tensor.dtype)\n",
    "        classes_weight += classes_w_tensor\n",
    "\n",
    "        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)\n",
    "\n",
    "        #3# get balanced focal loss\n",
    "        balanced_fl = alpha * FT\n",
    "        balanced_fl = tf.reduce_sum(balanced_fl)\n",
    "\n",
    "        #4# add other op to prevent overfit\n",
    "        # reference : https://spaces.ac.cn/archives/4493\n",
    "        nb_classes = len(classes_num)\n",
    "        fianal_loss = (1-e) * balanced_fl + e * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)\n",
    "\n",
    "        return fianal_loss\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape1 = (32, 32, 8)\n",
    "input_shape2 = (32, 32, 10)\n",
    "input_shape3 = (32, 32, 18)\n",
    "input_1 = Input(shape=input_shape1)\n",
    "input_2 = Input(shape=input_shape2)\n",
    "input_3 = Input(shape=input_shape3)\n",
    "L1 = BatchNormalization()(input_1)\n",
    "L2 = BatchNormalization()(input_2)\n",
    "L3 = BatchNormalization()(input_3)\n",
    "L1 = resnet_v2(input_tensor=L1, depth=92)\n",
    "L2 = resnet_v2(input_tensor=L2, depth=92)\n",
    "L3 = resnet_v2(input_tensor=L3, depth=92)\n",
    "\n",
    "#L1 = Conv2D(16, kernel_size=3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(L1)\n",
    "#L2 = Conv2D(16, kernel_size=3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(L2)\n",
    "\n",
    "\n",
    "L = keras.layers.concatenate([L1, L2, L3])\n",
    "L = Dense(512, activation='relu')(L)\n",
    "L = BatchNormalization()(L)\n",
    "L = Dropout(0.5)(L)\n",
    "L = Dense(17, activation='softmax', kernel_initializer='he_normal')(L)\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=L)\n",
    "class_nums = [5068, 24431, 31693,  8651, 16493, 35290,  3269, 39326,\n",
    "       13584, 11954, 42902,  9514,  9165, 41377,  2392,  7898,\n",
    "       49359]\n",
    "model.compile(loss=[focal_loss(class_nums)], optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = './models'\n",
    "model_name = 'LCZ_model_3_input.h5'\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc',save_best_only=True, save_weights_only=True)\n",
    "lr_reducr = ReduceLROnPlateau(factor=0.1,cooldown=0, patience=3, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor='val_acc',patience=5)\n",
    "calls = [checkpoint, lr_reducr, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = data_generate('./training.h5', batch_size=128, argumentation=True, shuffle=True)\n",
    "val_gen = data_generate('./validation.h5', batch_size=128, argumentation=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2753/2753 [==============================] - 2514s 913ms/step - loss: 4.4128 - acc: 0.7010 - val_loss: 7.8367 - val_acc: 0.5389\n",
      "Epoch 2/200\n",
      "2753/2753 [==============================] - 2368s 860ms/step - loss: 2.2667 - acc: 0.7767 - val_loss: 5.2554 - val_acc: 0.5782\n",
      "Epoch 3/200\n",
      "2753/2753 [==============================] - 2352s 854ms/step - loss: 1.8800 - acc: 0.8045 - val_loss: 5.8062 - val_acc: 0.5708\n",
      "Epoch 4/200\n",
      "2753/2753 [==============================] - 2342s 851ms/step - loss: 1.6898 - acc: 0.8227 - val_loss: 5.3533 - val_acc: 0.5555\n",
      "Epoch 5/200\n",
      "2753/2753 [==============================] - 2349s 853ms/step - loss: 1.5758 - acc: 0.8339 - val_loss: 4.6388 - val_acc: 0.5758\n",
      "Epoch 6/200\n",
      "2753/2753 [==============================] - 2338s 849ms/step - loss: 1.4901 - acc: 0.8432 - val_loss: 6.3270 - val_acc: 0.6403\n",
      "Epoch 7/200\n",
      "2753/2753 [==============================] - 2342s 851ms/step - loss: 1.4208 - acc: 0.8516 - val_loss: 4.6194 - val_acc: 0.6057\n",
      "Epoch 8/200\n",
      "2753/2753 [==============================] - 2336s 849ms/step - loss: 1.3704 - acc: 0.8573 - val_loss: 5.7836 - val_acc: 0.5715\n",
      "Epoch 9/200\n",
      "2753/2753 [==============================] - 2343s 851ms/step - loss: 1.3231 - acc: 0.8641 - val_loss: 4.9155 - val_acc: 0.6208\n",
      "Epoch 10/200\n",
      "2753/2753 [==============================] - 2346s 852ms/step - loss: 1.2965 - acc: 0.8675 - val_loss: 5.2990 - val_acc: 0.5677\n",
      "Epoch 11/200\n",
      "2753/2753 [==============================] - 2346s 852ms/step - loss: 1.2670 - acc: 0.8719 - val_loss: 5.6314 - val_acc: 0.5717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff03444b2e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, steps_per_epoch=352366//128+1, epochs=200,  verbose=1,callbacks=calls, validation_data=val_gen,\n",
    "                   validation_steps=24119//128+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generate2(data1, data2, data3, batch_size, argumentation = True, shuffle=False):\n",
    "    while True:\n",
    "        data_len = data1.shape[0]\n",
    "        c = [i for i in range(int(data_len/batch_size) + 1)]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(c)\n",
    "        for i in c:\n",
    "            t = np.random.rand()\n",
    "            try:\n",
    "                y_b = data3[i*batch_size:(i+1)*batch_size]\n",
    "                x_b1 = data1[i * batch_size: (i+1)*batch_size]\n",
    "                x_b2 = data2[i * batch_size: (i+1)*batch_size]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)\n",
    "            except:\n",
    "                y_b = data3[i*batch_size:]\n",
    "                x_b1 = data1[i*batch_size:]\n",
    "                x_b2 = data2[i*batch_size:]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid = h5py.File('./validation.h5', 'r')\n",
    "valid_s1 = valid['sen1']\n",
    "valid_s2 = valid['sen2']\n",
    "valid_label = valid['label']\n",
    "valid_s1 = np.array(valid_s1)\n",
    "valid_s2 = np.array(valid_s2)\n",
    "valid_label = np.array(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24119, 32, 32, 8), (24119, 32, 32, 10), (24119, 17))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_s1.shape,valid_s2.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018)\n",
    "kf = KFold(n_splits=5, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.argmax(valid_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = h5py.File('./round2_test_a_20190121.h5')\n",
    "test_s1 = np.array(test['sen1'])\n",
    "test_s2 = np.array(test['sen2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19295.2, 4823.8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24119*0.8,24119*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 236s 2s/step - loss: 2.9846 - acc: 0.6939 - val_loss: 2.2629 - val_acc: 0.7878\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 122s 805ms/step - loss: 2.1662 - acc: 0.7712 - val_loss: 1.9288 - val_acc: 0.8145\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 122s 811ms/step - loss: 1.9271 - acc: 0.8003 - val_loss: 1.7681 - val_acc: 0.8342\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 122s 809ms/step - loss: 1.7898 - acc: 0.8148 - val_loss: 1.6194 - val_acc: 0.8477\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 122s 811ms/step - loss: 1.6981 - acc: 0.8281 - val_loss: 1.5510 - val_acc: 0.8563\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.5957 - acc: 0.8401 - val_loss: 1.4905 - val_acc: 0.8700\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 122s 811ms/step - loss: 1.5535 - acc: 0.8483 - val_loss: 1.4357 - val_acc: 0.8717\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 122s 809ms/step - loss: 1.4982 - acc: 0.8562 - val_loss: 1.3988 - val_acc: 0.8760\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 122s 810ms/step - loss: 1.4366 - acc: 0.8634 - val_loss: 1.3893 - val_acc: 0.8868\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 122s 811ms/step - loss: 1.4092 - acc: 0.8701 - val_loss: 1.3331 - val_acc: 0.8893\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 1.3685 - acc: 0.8747 - val_loss: 1.3012 - val_acc: 0.8826\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.3396 - acc: 0.8830 - val_loss: 1.2709 - val_acc: 0.9011\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 1.3095 - acc: 0.8851 - val_loss: 1.2564 - val_acc: 0.8988\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 123s 814ms/step - loss: 1.2812 - acc: 0.8882 - val_loss: 1.2758 - val_acc: 0.9066\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 1.2531 - acc: 0.8957 - val_loss: 1.2091 - val_acc: 0.9046\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 122s 810ms/step - loss: 1.2376 - acc: 0.8954 - val_loss: 1.2073 - val_acc: 0.9079\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.2115 - acc: 0.8993 - val_loss: 1.2209 - val_acc: 0.9104\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1943 - acc: 0.9033 - val_loss: 1.1938 - val_acc: 0.9160\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 123s 811ms/step - loss: 1.1607 - acc: 0.9083 - val_loss: 1.1849 - val_acc: 0.9170\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1449 - acc: 0.9146 - val_loss: 1.1334 - val_acc: 0.9205\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 1.1417 - acc: 0.9134 - val_loss: 1.2028 - val_acc: 0.9178\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 1.1189 - acc: 0.9158 - val_loss: 1.1547 - val_acc: 0.9170\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.0981 - acc: 0.9213 - val_loss: 1.1583 - val_acc: 0.9236\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0887 - acc: 0.9204 - val_loss: 1.1216 - val_acc: 0.9174\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0653 - acc: 0.9258 - val_loss: 1.1275 - val_acc: 0.9199\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 122s 810ms/step - loss: 1.0667 - acc: 0.9271 - val_loss: 1.1124 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0507 - acc: 0.9295 - val_loss: 1.1418 - val_acc: 0.9329\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0407 - acc: 0.9306 - val_loss: 1.1145 - val_acc: 0.9313\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0361 - acc: 0.9339 - val_loss: 1.1434 - val_acc: 0.9317\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0083 - acc: 0.9398 - val_loss: 1.0794 - val_acc: 0.9348\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0028 - acc: 0.9397 - val_loss: 1.1013 - val_acc: 0.9323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f9ea7f97db0>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f9e900c0048>\n",
      "RuntimeError: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 141s 931ms/step - loss: 2.9953 - acc: 0.6955 - val_loss: 2.1639 - val_acc: 0.7691\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 122s 811ms/step - loss: 2.1845 - acc: 0.7701 - val_loss: 1.8100 - val_acc: 0.8165\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.9648 - acc: 0.7951 - val_loss: 1.6736 - val_acc: 0.8296\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.7997 - acc: 0.8104 - val_loss: 1.6219 - val_acc: 0.8354\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.6966 - acc: 0.8270 - val_loss: 1.4886 - val_acc: 0.8542\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.6323 - acc: 0.8366 - val_loss: 1.4723 - val_acc: 0.8590\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 123s 814ms/step - loss: 1.5540 - acc: 0.8480 - val_loss: 1.3820 - val_acc: 0.8720\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.5039 - acc: 0.8534 - val_loss: 1.3598 - val_acc: 0.8807\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.4623 - acc: 0.8623 - val_loss: 1.3244 - val_acc: 0.8861\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 121s 798ms/step - loss: 1.3927 - acc: 0.8718 - val_loss: 1.3129 - val_acc: 0.8824\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.3782 - acc: 0.8711 - val_loss: 1.2646 - val_acc: 0.8888\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.3401 - acc: 0.8793 - val_loss: 1.2345 - val_acc: 0.9004\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.3085 - acc: 0.8876 - val_loss: 1.2090 - val_acc: 0.9041\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 121s 798ms/step - loss: 1.2840 - acc: 0.8874 - val_loss: 1.1991 - val_acc: 0.9014\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.2612 - acc: 0.8935 - val_loss: 1.1994 - val_acc: 0.9091\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.2253 - acc: 0.8977 - val_loss: 1.1754 - val_acc: 0.9114\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.2167 - acc: 0.8990 - val_loss: 1.2152 - val_acc: 0.9124\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1824 - acc: 0.9060 - val_loss: 1.1398 - val_acc: 0.9172\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 122s 811ms/step - loss: 1.1705 - acc: 0.9078 - val_loss: 1.1520 - val_acc: 0.9197\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1624 - acc: 0.9103 - val_loss: 1.1213 - val_acc: 0.9223\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 123s 814ms/step - loss: 1.1379 - acc: 0.9138 - val_loss: 1.1009 - val_acc: 0.9226\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1240 - acc: 0.9149 - val_loss: 1.1241 - val_acc: 0.9228\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.1103 - acc: 0.9193 - val_loss: 1.1136 - val_acc: 0.9261\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.1051 - acc: 0.9203 - val_loss: 1.0863 - val_acc: 0.9333\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.0802 - acc: 0.9257 - val_loss: 1.0712 - val_acc: 0.9341\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 1.0700 - acc: 0.9272 - val_loss: 1.0958 - val_acc: 0.9246\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 1.0550 - acc: 0.9279 - val_loss: 1.0947 - val_acc: 0.9248\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 1.0410 - acc: 0.9341 - val_loss: 1.0590 - val_acc: 0.9335\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 121s 798ms/step - loss: 1.0477 - acc: 0.9319 - val_loss: 1.0676 - val_acc: 0.9273\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.0268 - acc: 0.9348 - val_loss: 1.0170 - val_acc: 0.9414\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 1.0137 - acc: 0.9364 - val_loss: 1.0530 - val_acc: 0.9352\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9964 - acc: 0.9426 - val_loss: 1.0551 - val_acc: 0.9294\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9989 - acc: 0.9412 - val_loss: 1.0338 - val_acc: 0.9412\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 126s 834ms/step - loss: 0.9843 - acc: 0.9436 - val_loss: 1.0364 - val_acc: 0.9426\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9671 - acc: 0.9490 - val_loss: 1.0037 - val_acc: 0.9412\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.9494 - acc: 0.9511 - val_loss: 0.9953 - val_acc: 0.9466\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9497 - acc: 0.9506 - val_loss: 0.9941 - val_acc: 0.9460\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9491 - acc: 0.9524 - val_loss: 1.0033 - val_acc: 0.9447\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9481 - acc: 0.9515 - val_loss: 1.0080 - val_acc: 0.9428\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 121s 798ms/step - loss: 0.9467 - acc: 0.9517 - val_loss: 1.0087 - val_acc: 0.9455\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.9480 - acc: 0.9518 - val_loss: 0.9871 - val_acc: 0.9495\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9494 - acc: 0.9516 - val_loss: 1.0048 - val_acc: 0.9468\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9427 - acc: 0.9535 - val_loss: 0.9978 - val_acc: 0.9466\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9410 - acc: 0.9541 - val_loss: 0.9880 - val_acc: 0.9433\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9480 - acc: 0.9509 - val_loss: 0.9910 - val_acc: 0.9470\n",
      "Epoch 46/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9490 - acc: 0.9510 - val_loss: 1.0000 - val_acc: 0.9457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f9c2fa68d58>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f9ea7f97db0>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 144s 955ms/step - loss: 3.0030 - acc: 0.6922 - val_loss: 2.2689 - val_acc: 0.7750\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 123s 817ms/step - loss: 2.1695 - acc: 0.7660 - val_loss: 1.9070 - val_acc: 0.8138\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.9249 - acc: 0.7950 - val_loss: 1.7668 - val_acc: 0.8318\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 1.7973 - acc: 0.8069 - val_loss: 1.6642 - val_acc: 0.8443\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.6798 - acc: 0.8241 - val_loss: 1.5622 - val_acc: 0.8544\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.5944 - acc: 0.8360 - val_loss: 1.5037 - val_acc: 0.8621\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.5621 - acc: 0.8419 - val_loss: 1.4434 - val_acc: 0.8737\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.4936 - acc: 0.8520 - val_loss: 1.4150 - val_acc: 0.8803\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.4588 - acc: 0.8599 - val_loss: 1.3627 - val_acc: 0.8917\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.4031 - acc: 0.8703 - val_loss: 1.3424 - val_acc: 0.8907\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.3608 - acc: 0.8740 - val_loss: 1.3318 - val_acc: 0.8868\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 124s 821ms/step - loss: 1.3314 - acc: 0.8813 - val_loss: 1.3277 - val_acc: 0.8949\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.3076 - acc: 0.8828 - val_loss: 1.3094 - val_acc: 0.9011\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.2836 - acc: 0.8860 - val_loss: 1.2676 - val_acc: 0.8998\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.2719 - acc: 0.8918 - val_loss: 1.2316 - val_acc: 0.9042\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 1.2362 - acc: 0.8969 - val_loss: 1.2291 - val_acc: 0.9048\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.2125 - acc: 0.8992 - val_loss: 1.2316 - val_acc: 0.9071\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 123s 817ms/step - loss: 1.1920 - acc: 0.9021 - val_loss: 1.2120 - val_acc: 0.9139\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.1811 - acc: 0.9076 - val_loss: 1.2212 - val_acc: 0.9056\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.1559 - acc: 0.9104 - val_loss: 1.1890 - val_acc: 0.9125\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.1333 - acc: 0.9125 - val_loss: 1.2017 - val_acc: 0.9200\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 1.1322 - acc: 0.9159 - val_loss: 1.1794 - val_acc: 0.9212\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.1162 - acc: 0.9172 - val_loss: 1.1407 - val_acc: 0.9258\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.1099 - acc: 0.9207 - val_loss: 1.1501 - val_acc: 0.9216\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0864 - acc: 0.9237 - val_loss: 1.1466 - val_acc: 0.9249\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 123s 814ms/step - loss: 1.0793 - acc: 0.9254 - val_loss: 1.1338 - val_acc: 0.9299\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 123s 814ms/step - loss: 1.0508 - acc: 0.9310 - val_loss: 1.1199 - val_acc: 0.9316\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0477 - acc: 0.9331 - val_loss: 1.1178 - val_acc: 0.9295\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.0440 - acc: 0.9335 - val_loss: 1.1031 - val_acc: 0.9332\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.0371 - acc: 0.9343 - val_loss: 1.1097 - val_acc: 0.9345\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 1.0182 - acc: 0.9371 - val_loss: 1.1179 - val_acc: 0.9338\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 1.0118 - acc: 0.9389 - val_loss: 1.1332 - val_acc: 0.9289\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 128s 846ms/step - loss: 0.9983 - acc: 0.9409 - val_loss: 1.1184 - val_acc: 0.9382\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 123s 817ms/step - loss: 0.9800 - acc: 0.9452 - val_loss: 1.0924 - val_acc: 0.9388\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 0.9724 - acc: 0.9481 - val_loss: 1.0645 - val_acc: 0.9413\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 0.9582 - acc: 0.9505 - val_loss: 1.0555 - val_acc: 0.9419\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 121s 799ms/step - loss: 0.9591 - acc: 0.9507 - val_loss: 1.0541 - val_acc: 0.9407\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.9625 - acc: 0.9500 - val_loss: 1.0740 - val_acc: 0.9382\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 124s 818ms/step - loss: 0.9634 - acc: 0.9476 - val_loss: 1.0638 - val_acc: 0.9438\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.9605 - acc: 0.9496 - val_loss: 1.0567 - val_acc: 0.9411\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.9532 - acc: 0.9505 - val_loss: 1.0711 - val_acc: 0.9388\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.9477 - acc: 0.9511 - val_loss: 1.0648 - val_acc: 0.9413\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 0.9558 - acc: 0.9507 - val_loss: 1.0487 - val_acc: 0.9471\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.9500 - acc: 0.9526 - val_loss: 1.0525 - val_acc: 0.9442\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.9503 - acc: 0.9518 - val_loss: 1.0731 - val_acc: 0.9419\n",
      "Epoch 46/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.9477 - acc: 0.9527 - val_loss: 1.0656 - val_acc: 0.9405\n",
      "Epoch 47/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.9498 - acc: 0.9517 - val_loss: 1.0633 - val_acc: 0.9419\n",
      "Epoch 48/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.9532 - acc: 0.9518 - val_loss: 1.0583 - val_acc: 0.9423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f9ea7fb3c50>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f9c2fa68d58>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 152s 1s/step - loss: 2.9223 - acc: 0.6942 - val_loss: 2.1929 - val_acc: 0.7810\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 2.1432 - acc: 0.7703 - val_loss: 1.8798 - val_acc: 0.8094\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.9300 - acc: 0.7956 - val_loss: 1.7284 - val_acc: 0.8270\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 125s 830ms/step - loss: 1.7866 - acc: 0.8108 - val_loss: 1.6226 - val_acc: 0.8426\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.6929 - acc: 0.8229 - val_loss: 1.5252 - val_acc: 0.8525\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.6073 - acc: 0.8380 - val_loss: 1.4491 - val_acc: 0.8693\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.5418 - acc: 0.8478 - val_loss: 1.4158 - val_acc: 0.8718\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.4928 - acc: 0.8512 - val_loss: 1.3987 - val_acc: 0.8803\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.4450 - acc: 0.8617 - val_loss: 1.3541 - val_acc: 0.8878\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 125s 830ms/step - loss: 1.4079 - acc: 0.8667 - val_loss: 1.3295 - val_acc: 0.8965\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 125s 830ms/step - loss: 1.3633 - acc: 0.8738 - val_loss: 1.2828 - val_acc: 0.8971\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.3439 - acc: 0.8779 - val_loss: 1.2637 - val_acc: 0.9029\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.3091 - acc: 0.8821 - val_loss: 1.2336 - val_acc: 0.9044\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 125s 830ms/step - loss: 1.2890 - acc: 0.8887 - val_loss: 1.2435 - val_acc: 0.9071\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.2655 - acc: 0.8905 - val_loss: 1.2103 - val_acc: 0.9118\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.2410 - acc: 0.8921 - val_loss: 1.1793 - val_acc: 0.9150\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.2296 - acc: 0.8982 - val_loss: 1.2231 - val_acc: 0.8938\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.1944 - acc: 0.9026 - val_loss: 1.1726 - val_acc: 0.9170\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.1768 - acc: 0.9061 - val_loss: 1.1717 - val_acc: 0.9220\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1610 - acc: 0.9081 - val_loss: 1.1707 - val_acc: 0.9212\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1428 - acc: 0.9122 - val_loss: 1.1603 - val_acc: 0.9203\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1253 - acc: 0.9150 - val_loss: 1.1493 - val_acc: 0.9197\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 125s 830ms/step - loss: 1.1230 - acc: 0.9174 - val_loss: 1.1238 - val_acc: 0.9278\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.0987 - acc: 0.9222 - val_loss: 1.1783 - val_acc: 0.9199\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.0831 - acc: 0.9224 - val_loss: 1.1171 - val_acc: 0.9274\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 125s 830ms/step - loss: 1.0792 - acc: 0.9248 - val_loss: 1.1354 - val_acc: 0.9280\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.0588 - acc: 0.9283 - val_loss: 1.1094 - val_acc: 0.9326\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 125s 831ms/step - loss: 1.0466 - acc: 0.9303 - val_loss: 1.1327 - val_acc: 0.9374\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.0268 - acc: 0.9348 - val_loss: 1.0943 - val_acc: 0.9371\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.0313 - acc: 0.9360 - val_loss: 1.1070 - val_acc: 0.9351\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 1.0081 - acc: 0.9376 - val_loss: 1.0821 - val_acc: 0.9384\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 0.9984 - acc: 0.9414 - val_loss: 1.0949 - val_acc: 0.9390\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 0.9964 - acc: 0.9417 - val_loss: 1.0984 - val_acc: 0.9409\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 0.9936 - acc: 0.9429 - val_loss: 1.0709 - val_acc: 0.9436\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.9719 - acc: 0.9457 - val_loss: 1.0890 - val_acc: 0.9409\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 125s 830ms/step - loss: 0.9639 - acc: 0.9480 - val_loss: 1.0702 - val_acc: 0.9446\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 125s 831ms/step - loss: 0.9722 - acc: 0.9458 - val_loss: 1.0356 - val_acc: 0.9461\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.9577 - acc: 0.9480 - val_loss: 1.0535 - val_acc: 0.9448\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.9438 - acc: 0.9527 - val_loss: 1.0449 - val_acc: 0.9363\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.9407 - acc: 0.9543 - val_loss: 1.0588 - val_acc: 0.9440\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 131s 869ms/step - loss: 0.9375 - acc: 0.9545 - val_loss: 1.0380 - val_acc: 0.9517\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.9158 - acc: 0.9586 - val_loss: 1.0235 - val_acc: 0.9479\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 0.9038 - acc: 0.9622 - val_loss: 1.0091 - val_acc: 0.9523\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.8993 - acc: 0.9618 - val_loss: 1.0293 - val_acc: 0.9523\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 125s 830ms/step - loss: 0.9116 - acc: 0.9599 - val_loss: 0.9988 - val_acc: 0.9525\n",
      "Epoch 46/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.9055 - acc: 0.9613 - val_loss: 1.0231 - val_acc: 0.9519\n",
      "Epoch 47/200\n",
      "151/151 [==============================] - 125s 830ms/step - loss: 0.9042 - acc: 0.9616 - val_loss: 0.9948 - val_acc: 0.9531\n",
      "Epoch 48/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.8995 - acc: 0.9614 - val_loss: 1.0019 - val_acc: 0.9500\n",
      "Epoch 49/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 0.9035 - acc: 0.9614 - val_loss: 1.0388 - val_acc: 0.9496\n",
      "Epoch 50/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.8996 - acc: 0.9635 - val_loss: 1.0007 - val_acc: 0.9529\n",
      "Epoch 51/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.8944 - acc: 0.9630 - val_loss: 1.0159 - val_acc: 0.9521\n",
      "Epoch 52/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.8954 - acc: 0.9644 - val_loss: 1.0250 - val_acc: 0.9527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f9ca96bfaf0>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f9ea7fb3c50>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 155s 1s/step - loss: 2.9664 - acc: 0.6935 - val_loss: 2.1966 - val_acc: 0.7824\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 126s 831ms/step - loss: 2.1444 - acc: 0.7702 - val_loss: 1.8767 - val_acc: 0.8196\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 126s 836ms/step - loss: 1.9101 - acc: 0.7981 - val_loss: 1.7260 - val_acc: 0.8418\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.7637 - acc: 0.8166 - val_loss: 1.6014 - val_acc: 0.8372\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.6684 - acc: 0.8265 - val_loss: 1.5320 - val_acc: 0.8540\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.5935 - acc: 0.8388 - val_loss: 1.4680 - val_acc: 0.8571\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 1.5264 - acc: 0.8469 - val_loss: 1.4496 - val_acc: 0.8698\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 1.4775 - acc: 0.8573 - val_loss: 1.4212 - val_acc: 0.8725\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.4284 - acc: 0.8653 - val_loss: 1.3930 - val_acc: 0.8792\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.3995 - acc: 0.8700 - val_loss: 1.3348 - val_acc: 0.8854\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.3649 - acc: 0.8734 - val_loss: 1.2859 - val_acc: 0.8852\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.3209 - acc: 0.8798 - val_loss: 1.3201 - val_acc: 0.8908\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.2919 - acc: 0.8844 - val_loss: 1.2659 - val_acc: 0.8964\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 1.2671 - acc: 0.8884 - val_loss: 1.2835 - val_acc: 0.8997\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.2517 - acc: 0.8937 - val_loss: 1.2577 - val_acc: 0.8983\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.2054 - acc: 0.8996 - val_loss: 1.2351 - val_acc: 0.9053\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1913 - acc: 0.9052 - val_loss: 1.2221 - val_acc: 0.9047\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.1815 - acc: 0.9049 - val_loss: 1.1947 - val_acc: 0.9118\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.1592 - acc: 0.9084 - val_loss: 1.1824 - val_acc: 0.9203\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.1575 - acc: 0.9123 - val_loss: 1.2179 - val_acc: 0.9053\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.1241 - acc: 0.9161 - val_loss: 1.1607 - val_acc: 0.9217\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.1308 - acc: 0.9167 - val_loss: 1.1605 - val_acc: 0.9151\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 1.1129 - acc: 0.9182 - val_loss: 1.1421 - val_acc: 0.9194\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.0816 - acc: 0.9237 - val_loss: 1.1739 - val_acc: 0.9225\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.0726 - acc: 0.9272 - val_loss: 1.1641 - val_acc: 0.9228\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 1.0601 - acc: 0.9286 - val_loss: 1.1727 - val_acc: 0.9255\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 131s 865ms/step - loss: 1.0517 - acc: 0.9299 - val_loss: 1.1497 - val_acc: 0.9228\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.0265 - acc: 0.9337 - val_loss: 1.1184 - val_acc: 0.9331\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.0183 - acc: 0.9369 - val_loss: 1.0988 - val_acc: 0.9331\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 126s 832ms/step - loss: 1.0112 - acc: 0.9379 - val_loss: 1.1203 - val_acc: 0.9333\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 1.0091 - acc: 0.9380 - val_loss: 1.0979 - val_acc: 0.9373\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.0125 - acc: 0.9378 - val_loss: 1.1110 - val_acc: 0.9340\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.0056 - acc: 0.9402 - val_loss: 1.1177 - val_acc: 0.9302\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 1.0049 - acc: 0.9398 - val_loss: 1.1100 - val_acc: 0.9327\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 123s 813ms/step - loss: 0.9955 - acc: 0.9400 - val_loss: 1.0968 - val_acc: 0.9350\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 123s 812ms/step - loss: 0.9878 - acc: 0.9419 - val_loss: 1.0975 - val_acc: 0.9331\n"
     ]
    }
   ],
   "source": [
    "pred = np.zeros((test_s1.shape[0], 17))\n",
    "test_s3 = np.concatenate([test_s1, test_s2], axis=-1)\n",
    "for i ,(tr_ind, val_ind) in enumerate(skf.split(valid_s1, y)):\n",
    "    tr1, tr2, trl = valid_s1[tr_ind], valid_s2[tr_ind], valid_label[tr_ind]\n",
    "    va1, va2, val = valid_s1[val_ind], valid_s2[val_ind], valid_label[val_ind]\n",
    "    save_dir = './model_2/'\n",
    "    model_name = 'LCZ_model_FOLD_3_input_%s.h5' % str(i)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc',save_best_only=True, save_weights_only=True)\n",
    "    lr_reducr = ReduceLROnPlateau(factor=0.1,cooldown=0, patience=3, min_lr=1e-6)\n",
    "    early = EarlyStopping(monitor='val_acc',patience=5,)\n",
    "    calls = [checkpoint, lr_reducr, early]\n",
    "    class_num = [ 256., 1254., 2353.,  849.,  757., 1906.,  474., 3395., 1914.,\n",
    "        860., 2287.,  382., 1202., 2747.,  202.,  672., 2609.]\n",
    "    class_num = [int(x*0.8) for x in class_num]\n",
    "    model.compile(loss=focal_loss(class_num), optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "    model.load_weights('./models/LCZ_model_3_input.h5')\n",
    "    tr_gen = data_generate2(tr1, tr2, trl,argumentation=True,batch_size=128,shuffle=True)\n",
    "    va_gen = data_generate2(va1, va2, val,argumentation=True,batch_size=128,shuffle=False)\n",
    "    model.fit_generator(tr_gen, steps_per_epoch=19295//128+1, epochs=200, validation_data=va_gen,validation_steps=4823//128+1, callbacks=calls)\n",
    "    model.load_weights(filepath)\n",
    "    for j in range(4):\n",
    "        te1 = np.rot90(test_s1,k=j,axes=(1,2))\n",
    "        te2 = np.rot90(test_s2,k=j,axes=(1,2))\n",
    "        te3 = np.rot90(test_s3,k=j,axes=(1,2))\n",
    "        pred += model.predict([te1, te2, te3])\n",
    "    te1 = test_s1[:,::-1,:,:]\n",
    "    te2 = test_s2[:,::-1,:,:]\n",
    "    te3 = test_s3[:,::-1,:,:]\n",
    "    pred += model.predict([te1, te2, te3])\n",
    "    te1 = test_s1[:,:,::-1,:]\n",
    "    te2 = test_s2[:,:,::-1,:]\n",
    "    te3 = test_s3[:,:,::-1,:]\n",
    "    pred += model.predict([te1, te2, te3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4842, 17)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pred / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.99999999, ..., 1.        , 1.        ,\n",
       "       0.99999999])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.zeros((4842, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i ,ind in enumerate(np.argmax(preds,axis=1)):\n",
    "    result[i][ind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.DataFrame(result,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.to_csv('round2_a1.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
