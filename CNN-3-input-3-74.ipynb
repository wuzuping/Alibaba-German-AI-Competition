{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense,Conv2D, BatchNormalization, Activation,Dropout\n",
    "from keras.layers import AveragePooling2D, Input, Flatten,GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import skimage\n",
    "from keras.models import Model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generate(data_path, batch_size, argumentation = True, shuffle=False):\n",
    "    while True:\n",
    "        fid = h5py.File(data_path, 'r')\n",
    "        data_len = fid['sen1'].shape[0]\n",
    "        c = [i for i in range(int(data_len/batch_size) + 1)]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(c)\n",
    "        for i in c:\n",
    "            t = np.random.rand()\n",
    "            try:\n",
    "                y_b = np.array((fid['label'][i * batch_size: (i+1)*batch_size]))\n",
    "                x_b1 = fid['sen1'][i * batch_size: (i+1)*batch_size]\n",
    "                x_b2 = fid['sen2'][i * batch_size: (i+1)*batch_size]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([1,2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)\n",
    "            except:\n",
    "                x_b1 = fid['sen1'][i*batch_size:]\n",
    "                x_b2 = fid['sen2'][i*batch_size:]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                y_b = np.array((fid['label'][i * batch_size:]))\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        \n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3] , y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_v2(input_tensor, depth, num_classes=17):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    #inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=input_tensor,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #y = Flatten()(x)\n",
    "    #outputs = Dense(num_classes,\n",
    "     #               activation='softmax',\n",
    "     #               kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    #model = Model(inputs=inputs, outputs=outputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "def focal_loss(classes_num, gamma=2., alpha=.25, e=0.1):\n",
    "    # classes_num contains sample number of each classes\n",
    "    def focal_loss_fixed(target_tensor, prediction_tensor):\n",
    "        '''\n",
    "        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes\n",
    "        target_tensor is the label tensor, same shape as predcition_tensor\n",
    "        '''\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.python.ops import array_ops\n",
    "        from keras import backend as K\n",
    "\n",
    "        #1# get focal loss with no balanced weight which presented in paper function (4)\n",
    "        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
    "        one_minus_p = array_ops.where(tf.greater(target_tensor,zeros), target_tensor - prediction_tensor, zeros)\n",
    "        FT = -1 * (one_minus_p ** gamma) * tf.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))\n",
    "\n",
    "        #2# get balanced weight alpha\n",
    "        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
    "\n",
    "        total_num = float(sum(classes_num))\n",
    "        classes_w_t1 = [ total_num / ff for ff in classes_num ]\n",
    "        sum_ = sum(classes_w_t1)\n",
    "        classes_w_t2 = [ ff/sum_ for ff in classes_w_t1 ]   #scale\n",
    "        classes_w_tensor = tf.convert_to_tensor(classes_w_t2, dtype=prediction_tensor.dtype)\n",
    "        classes_weight += classes_w_tensor\n",
    "\n",
    "        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)\n",
    "\n",
    "        #3# get balanced focal loss\n",
    "        balanced_fl = alpha * FT\n",
    "        balanced_fl = tf.reduce_sum(balanced_fl)\n",
    "\n",
    "        #4# add other op to prevent overfit\n",
    "        # reference : https://spaces.ac.cn/archives/4493\n",
    "        nb_classes = len(classes_num)\n",
    "        fianal_loss = (1-e) * balanced_fl + e * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)\n",
    "\n",
    "        return fianal_loss\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape1 = (32, 32, 8)\n",
    "input_shape2 = (32, 32, 10)\n",
    "input_shape3 = (32, 32, 18)\n",
    "input_1 = Input(shape=input_shape1)\n",
    "input_2 = Input(shape=input_shape2)\n",
    "input_3 = Input(shape=input_shape3)\n",
    "L1 = BatchNormalization()(input_1)\n",
    "L2 = BatchNormalization()(input_2)\n",
    "L3 = BatchNormalization()(input_3)\n",
    "L1 = resnet_v2(input_tensor=L1, depth=74)\n",
    "L2 = resnet_v2(input_tensor=L2, depth=74)\n",
    "L3 = resnet_v2(input_tensor=L3, depth=74)\n",
    "\n",
    "#L1 = Conv2D(16, kernel_size=3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(L1)\n",
    "#L2 = Conv2D(16, kernel_size=3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(L2)\n",
    "\n",
    "\n",
    "L = keras.layers.concatenate([L1, L2, L3])\n",
    "L = Dense(512, activation='relu')(L)\n",
    "L = BatchNormalization()(L)\n",
    "L = Dropout(0.5)(L)\n",
    "L = Dense(17, activation='softmax', kernel_initializer='he_normal')(L)\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=L)\n",
    "class_nums = [5068, 24431, 31693,  8651, 16493, 35290,  3269, 39326,\n",
    "       13584, 11954, 42902,  9514,  9165, 41377,  2392,  7898,\n",
    "       49359]\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "model.compile(loss=focal_loss(class_nums), optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = './models'\n",
    "model_name = 'LCZ_model_3_inputsss-950-loss-f-74.h5'\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc',save_best_only=True, save_weights_only=True)\n",
    "lr_reducr = ReduceLROnPlateau(factor=0.1,cooldown=0, patience=3, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor='val_acc',patience=5)\n",
    "calls = [checkpoint, lr_reducr, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = data_generate('./training.h5', batch_size=128, argumentation=True, shuffle=True)\n",
    "val_gen = data_generate('./validation.h5', batch_size=128, argumentation=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2753/2753 [==============================] - 2113s 768ms/step - loss: 4.1524 - acc: 0.6983 - val_loss: 5.9140 - val_acc: 0.5225\n",
      "Epoch 2/200\n",
      "2753/2753 [==============================] - 2034s 739ms/step - loss: 2.2600 - acc: 0.7769 - val_loss: 5.4861 - val_acc: 0.5140\n",
      "Epoch 3/200\n",
      "2753/2753 [==============================] - 2023s 735ms/step - loss: 1.8751 - acc: 0.8068 - val_loss: 5.5416 - val_acc: 0.5729\n",
      "Epoch 4/200\n",
      "2753/2753 [==============================] - 2019s 733ms/step - loss: 1.6771 - acc: 0.8259 - val_loss: 5.5260 - val_acc: 0.5970\n",
      "Epoch 5/200\n",
      "2753/2753 [==============================] - 2018s 733ms/step - loss: 1.5576 - acc: 0.8372 - val_loss: 5.6625 - val_acc: 0.5669\n",
      "Epoch 6/200\n",
      "2753/2753 [==============================] - 2016s 732ms/step - loss: 1.4631 - acc: 0.8489 - val_loss: 6.1365 - val_acc: 0.5737\n",
      "Epoch 7/200\n",
      "2753/2753 [==============================] - 1994s 724ms/step - loss: 1.2420 - acc: 0.8842 - val_loss: 5.3060 - val_acc: 0.5958\n",
      "Epoch 8/200\n",
      "2753/2753 [==============================] - 1996s 725ms/step - loss: 1.1718 - acc: 0.8946 - val_loss: 5.7436 - val_acc: 0.6056\n",
      "Epoch 9/200\n",
      "2753/2753 [==============================] - 2001s 727ms/step - loss: 1.1305 - acc: 0.8998 - val_loss: 5.7524 - val_acc: 0.6067\n",
      "Epoch 10/200\n",
      "2753/2753 [==============================] - 1996s 725ms/step - loss: 1.1019 - acc: 0.9042 - val_loss: 5.8222 - val_acc: 0.6030\n",
      "Epoch 11/200\n",
      "2753/2753 [==============================] - 2002s 727ms/step - loss: 1.0757 - acc: 0.9080 - val_loss: 5.8668 - val_acc: 0.6028\n",
      "Epoch 12/200\n",
      "2753/2753 [==============================] - 1998s 726ms/step - loss: 1.0355 - acc: 0.9153 - val_loss: 5.8783 - val_acc: 0.5970\n",
      "Epoch 13/200\n",
      "2753/2753 [==============================] - 2007s 729ms/step - loss: 1.0312 - acc: 0.9161 - val_loss: 5.9083 - val_acc: 0.5992\n",
      "Epoch 14/200\n",
      "2753/2753 [==============================] - 2012s 731ms/step - loss: 1.0266 - acc: 0.9172 - val_loss: 5.9157 - val_acc: 0.6006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26351535f8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, steps_per_epoch=352366//128+1, epochs=200,  verbose=1,callbacks=calls, validation_data=val_gen,\n",
    "                   validation_steps=24119//128+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generate2(data1, data2, data3, batch_size, argumentation = True, shuffle=False):\n",
    "    while True:\n",
    "        data_len = data1.shape[0]\n",
    "        c = [i for i in range(int(data_len/batch_size) + 1)]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(c)\n",
    "        for i in c:\n",
    "            t = np.random.rand()\n",
    "            try:\n",
    "                y_b = data3[i*batch_size:(i+1)*batch_size]\n",
    "                x_b1 = data1[i * batch_size: (i+1)*batch_size]\n",
    "                x_b2 = data2[i * batch_size: (i+1)*batch_size]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)\n",
    "            except:\n",
    "                y_b = data3[i*batch_size:]\n",
    "                x_b1 = data1[i*batch_size:]\n",
    "                x_b2 = data2[i*batch_size:]\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            x_b3 = x_b3[:,:,::-1,:]\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid = h5py.File('./validation.h5', 'r')\n",
    "valid_s1 = valid['sen1']\n",
    "valid_s2 = valid['sen2']\n",
    "valid_label = valid['label']\n",
    "valid_s1 = np.array(valid_s1)\n",
    "valid_s2 = np.array(valid_s2)\n",
    "valid_label = np.array(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24119, 32, 32, 8), (24119, 32, 32, 10), (24119, 17))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_s1.shape,valid_s2.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018)\n",
    "kf = KFold(n_splits=5, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.argmax(valid_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = h5py.File('./round2_test_a_20190121.h5')\n",
    "test_s1 = np.array(test['sen1'])\n",
    "test_s2 = np.array(test['sen2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19295.2, 4823.8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24119*0.8,24119*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 113s 746ms/step - loss: 2.8139 - acc: 0.7165 - val_loss: 1.9154 - val_acc: 0.7814\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 99s 656ms/step - loss: 1.8913 - acc: 0.7925 - val_loss: 1.5848 - val_acc: 0.8245\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 100s 660ms/step - loss: 1.6677 - acc: 0.8198 - val_loss: 1.4361 - val_acc: 0.8518\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 99s 657ms/step - loss: 1.5231 - acc: 0.8370 - val_loss: 1.3290 - val_acc: 0.8698\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 100s 660ms/step - loss: 1.4347 - acc: 0.8516 - val_loss: 1.2965 - val_acc: 0.8791\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 99s 658ms/step - loss: 1.3527 - acc: 0.8646 - val_loss: 1.2335 - val_acc: 0.8876\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 99s 658ms/step - loss: 1.3060 - acc: 0.8749 - val_loss: 1.2038 - val_acc: 0.8963\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 100s 660ms/step - loss: 1.2569 - acc: 0.8834 - val_loss: 1.1584 - val_acc: 0.9025\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 99s 659ms/step - loss: 1.2102 - acc: 0.8882 - val_loss: 1.1498 - val_acc: 0.9058\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 99s 657ms/step - loss: 1.1891 - acc: 0.8957 - val_loss: 1.1519 - val_acc: 0.9116\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 99s 658ms/step - loss: 1.1480 - acc: 0.9020 - val_loss: 1.1037 - val_acc: 0.9160\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 99s 658ms/step - loss: 1.1293 - acc: 0.9038 - val_loss: 1.1163 - val_acc: 0.9195\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 98s 648ms/step - loss: 1.1023 - acc: 0.9081 - val_loss: 1.0979 - val_acc: 0.9191\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 99s 658ms/step - loss: 1.0783 - acc: 0.9158 - val_loss: 1.1050 - val_acc: 0.9224\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 99s 657ms/step - loss: 1.0596 - acc: 0.9199 - val_loss: 1.0452 - val_acc: 0.9276\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 99s 657ms/step - loss: 1.0421 - acc: 0.9227 - val_loss: 1.0654 - val_acc: 0.9294\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 99s 655ms/step - loss: 1.0107 - acc: 0.9261 - val_loss: 1.0500 - val_acc: 0.9271\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 100s 659ms/step - loss: 1.0163 - acc: 0.9285 - val_loss: 1.0403 - val_acc: 0.9352\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 98s 649ms/step - loss: 0.9995 - acc: 0.9311 - val_loss: 1.0244 - val_acc: 0.9342\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.9789 - acc: 0.9350 - val_loss: 1.0281 - val_acc: 0.9350\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 99s 658ms/step - loss: 0.9564 - acc: 0.9398 - val_loss: 1.0101 - val_acc: 0.9402\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 98s 648ms/step - loss: 0.9426 - acc: 0.9441 - val_loss: 1.0371 - val_acc: 0.9391\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 98s 649ms/step - loss: 0.9440 - acc: 0.9452 - val_loss: 0.9965 - val_acc: 0.9398\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 99s 652ms/step - loss: 0.9267 - acc: 0.9477 - val_loss: 1.0165 - val_acc: 0.9375\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 100s 660ms/step - loss: 0.9203 - acc: 0.9499 - val_loss: 1.0044 - val_acc: 0.9422\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 0.9082 - acc: 0.9512 - val_loss: 0.9915 - val_acc: 0.9410\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 0.9072 - acc: 0.9507 - val_loss: 1.0069 - val_acc: 0.9445\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 100s 661ms/step - loss: 0.8792 - acc: 0.9572 - val_loss: 0.9679 - val_acc: 0.9489\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 98s 649ms/step - loss: 0.8714 - acc: 0.9609 - val_loss: 0.9812 - val_acc: 0.9458\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 98s 649ms/step - loss: 0.8692 - acc: 0.9605 - val_loss: 0.9722 - val_acc: 0.9480\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 0.8800 - acc: 0.9596 - val_loss: 0.9750 - val_acc: 0.9480\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 102s 677ms/step - loss: 0.8589 - acc: 0.9625 - val_loss: 0.9719 - val_acc: 0.9503\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 99s 658ms/step - loss: 0.8425 - acc: 0.9656 - val_loss: 0.9496 - val_acc: 0.9563\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 100s 661ms/step - loss: 0.8324 - acc: 0.9684 - val_loss: 0.9627 - val_acc: 0.9572\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 98s 649ms/step - loss: 0.8303 - acc: 0.9700 - val_loss: 0.9607 - val_acc: 0.9551\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 100s 660ms/step - loss: 0.8259 - acc: 0.9714 - val_loss: 0.9406 - val_acc: 0.9594\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 98s 648ms/step - loss: 0.8287 - acc: 0.9704 - val_loss: 0.9416 - val_acc: 0.9584\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 98s 648ms/step - loss: 0.8279 - acc: 0.9713 - val_loss: 0.9530 - val_acc: 0.9586\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 98s 648ms/step - loss: 0.8240 - acc: 0.9725 - val_loss: 0.9549 - val_acc: 0.9574\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 98s 648ms/step - loss: 0.8299 - acc: 0.9723 - val_loss: 0.9361 - val_acc: 0.9561\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 99s 658ms/step - loss: 0.8255 - acc: 0.9716 - val_loss: 0.9265 - val_acc: 0.9605\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.8233 - acc: 0.9729 - val_loss: 0.9449 - val_acc: 0.9580\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 98s 648ms/step - loss: 0.8211 - acc: 0.9732 - val_loss: 0.9587 - val_acc: 0.9557\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 98s 647ms/step - loss: 0.8243 - acc: 0.9725 - val_loss: 0.9285 - val_acc: 0.9578\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 98s 648ms/step - loss: 0.8176 - acc: 0.9735 - val_loss: 0.9485 - val_acc: 0.9586\n",
      "Epoch 46/200\n",
      "151/151 [==============================] - 98s 648ms/step - loss: 0.8178 - acc: 0.9723 - val_loss: 0.9410 - val_acc: 0.9580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f2635618f10>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f2580743e08>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 2.8385 - acc: 0.7174 - val_loss: 1.8889 - val_acc: 0.7633\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 1.8998 - acc: 0.7884 - val_loss: 1.5380 - val_acc: 0.8225\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 100s 664ms/step - loss: 1.6762 - acc: 0.8174 - val_loss: 1.4121 - val_acc: 0.8395\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 1.5289 - acc: 0.8333 - val_loss: 1.3266 - val_acc: 0.8629\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 1.4407 - acc: 0.8487 - val_loss: 1.2849 - val_acc: 0.8745\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 1.3571 - acc: 0.8655 - val_loss: 1.2216 - val_acc: 0.8896\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 1.3070 - acc: 0.8700 - val_loss: 1.2014 - val_acc: 0.9033\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 1.2657 - acc: 0.8800 - val_loss: 1.1890 - val_acc: 0.8981\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 100s 661ms/step - loss: 1.2201 - acc: 0.8904 - val_loss: 1.1317 - val_acc: 0.9116\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 1.2002 - acc: 0.8937 - val_loss: 1.1242 - val_acc: 0.9122\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 1.1521 - acc: 0.8991 - val_loss: 1.1099 - val_acc: 0.9188\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 1.1335 - acc: 0.9051 - val_loss: 1.1034 - val_acc: 0.9141\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 1.1141 - acc: 0.9087 - val_loss: 1.0720 - val_acc: 0.9219\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 1.0804 - acc: 0.9138 - val_loss: 1.0638 - val_acc: 0.9190\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 1.0714 - acc: 0.9160 - val_loss: 1.0557 - val_acc: 0.9213\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 1.0402 - acc: 0.9215 - val_loss: 1.0508 - val_acc: 0.9261\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 1.0275 - acc: 0.9250 - val_loss: 1.0206 - val_acc: 0.9319\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 1.0181 - acc: 0.9282 - val_loss: 1.0189 - val_acc: 0.9302\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.9993 - acc: 0.9301 - val_loss: 1.0235 - val_acc: 0.9337\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.9831 - acc: 0.9352 - val_loss: 1.0217 - val_acc: 0.9335\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 0.9737 - acc: 0.9365 - val_loss: 0.9853 - val_acc: 0.9344\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 0.9533 - acc: 0.9407 - val_loss: 1.0001 - val_acc: 0.9375\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 0.9434 - acc: 0.9447 - val_loss: 0.9921 - val_acc: 0.9393\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.9360 - acc: 0.9459 - val_loss: 0.9676 - val_acc: 0.9385\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.9271 - acc: 0.9468 - val_loss: 1.0087 - val_acc: 0.9387\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 100s 664ms/step - loss: 0.9182 - acc: 0.9512 - val_loss: 0.9573 - val_acc: 0.9445\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.9002 - acc: 0.9538 - val_loss: 0.9862 - val_acc: 0.9455\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.8945 - acc: 0.9541 - val_loss: 0.9705 - val_acc: 0.9468\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.8816 - acc: 0.9603 - val_loss: 0.9549 - val_acc: 0.9472\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.8880 - acc: 0.9566 - val_loss: 0.9727 - val_acc: 0.9455\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 0.8719 - acc: 0.9604 - val_loss: 0.9409 - val_acc: 0.9546\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.8580 - acc: 0.9631 - val_loss: 0.9396 - val_acc: 0.9536\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.8610 - acc: 0.9628 - val_loss: 0.9500 - val_acc: 0.9489\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.8415 - acc: 0.9670 - val_loss: 0.9318 - val_acc: 0.9524\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.8417 - acc: 0.9661 - val_loss: 0.9488 - val_acc: 0.9555\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 0.8435 - acc: 0.9645 - val_loss: 0.9512 - val_acc: 0.9569\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.8350 - acc: 0.9694 - val_loss: 0.9515 - val_acc: 0.9538\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.8352 - acc: 0.9681 - val_loss: 0.9243 - val_acc: 0.9470\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 0.8168 - acc: 0.9728 - val_loss: 0.9291 - val_acc: 0.9594\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.8170 - acc: 0.9735 - val_loss: 0.9237 - val_acc: 0.9571\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.8109 - acc: 0.9749 - val_loss: 0.9201 - val_acc: 0.9598\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.8076 - acc: 0.9748 - val_loss: 0.9149 - val_acc: 0.9582\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.8088 - acc: 0.9757 - val_loss: 0.9395 - val_acc: 0.9592\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 0.8012 - acc: 0.9776 - val_loss: 0.9079 - val_acc: 0.9611\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.7844 - acc: 0.9791 - val_loss: 0.9301 - val_acc: 0.9580\n",
      "Epoch 46/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.7883 - acc: 0.9787 - val_loss: 0.9260 - val_acc: 0.9596\n",
      "Epoch 47/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.7875 - acc: 0.9803 - val_loss: 0.9003 - val_acc: 0.9609\n",
      "Epoch 48/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.7815 - acc: 0.9819 - val_loss: 0.8913 - val_acc: 0.9619\n",
      "Epoch 49/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.7778 - acc: 0.9815 - val_loss: 0.9141 - val_acc: 0.9604\n",
      "Epoch 50/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.7787 - acc: 0.9807 - val_loss: 0.8900 - val_acc: 0.9604\n",
      "Epoch 51/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.7703 - acc: 0.9830 - val_loss: 0.8860 - val_acc: 0.9609\n",
      "Epoch 52/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.7677 - acc: 0.9831 - val_loss: 0.9023 - val_acc: 0.9638\n",
      "Epoch 53/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.7693 - acc: 0.9838 - val_loss: 0.9168 - val_acc: 0.9627\n",
      "Epoch 54/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.7633 - acc: 0.9846 - val_loss: 0.8780 - val_acc: 0.9598\n",
      "Epoch 55/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.7627 - acc: 0.9836 - val_loss: 0.8956 - val_acc: 0.9619\n",
      "Epoch 56/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 0.7600 - acc: 0.9837 - val_loss: 0.8819 - val_acc: 0.9679\n",
      "Epoch 57/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.7606 - acc: 0.9840 - val_loss: 0.8778 - val_acc: 0.9654\n",
      "Epoch 58/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.7488 - acc: 0.9876 - val_loss: 0.8809 - val_acc: 0.9660\n",
      "Epoch 59/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.7518 - acc: 0.9859 - val_loss: 0.8784 - val_acc: 0.9646\n",
      "Epoch 60/200\n",
      "151/151 [==============================] - 99s 652ms/step - loss: 0.7513 - acc: 0.9883 - val_loss: 0.8872 - val_acc: 0.9609\n",
      "Epoch 61/200\n",
      "151/151 [==============================] - 102s 676ms/step - loss: 0.7464 - acc: 0.9876 - val_loss: 0.8935 - val_acc: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f2495ee7728>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f2635618f10>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 120s 793ms/step - loss: 2.7448 - acc: 0.7173 - val_loss: 1.9963 - val_acc: 0.7684\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 1.8900 - acc: 0.7879 - val_loss: 1.6188 - val_acc: 0.8266\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 1.6552 - acc: 0.8159 - val_loss: 1.4741 - val_acc: 0.8407\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 1.5221 - acc: 0.8347 - val_loss: 1.4125 - val_acc: 0.8662\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 1.4480 - acc: 0.8514 - val_loss: 1.3525 - val_acc: 0.8571\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 100s 664ms/step - loss: 1.3750 - acc: 0.8564 - val_loss: 1.3056 - val_acc: 0.8808\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 1.3103 - acc: 0.8694 - val_loss: 1.2700 - val_acc: 0.8851\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 1.2657 - acc: 0.8770 - val_loss: 1.2102 - val_acc: 0.9019\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 100s 664ms/step - loss: 1.2176 - acc: 0.8838 - val_loss: 1.1917 - val_acc: 0.9065\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 1.1883 - acc: 0.8935 - val_loss: 1.1823 - val_acc: 0.9005\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 1.1566 - acc: 0.8967 - val_loss: 1.1649 - val_acc: 0.9073\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 1.1218 - acc: 0.9045 - val_loss: 1.1481 - val_acc: 0.9135\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 1.1053 - acc: 0.9064 - val_loss: 1.1302 - val_acc: 0.9170\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 100s 664ms/step - loss: 1.0783 - acc: 0.9142 - val_loss: 1.1159 - val_acc: 0.9224\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 1.0618 - acc: 0.9176 - val_loss: 1.0964 - val_acc: 0.9154\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 1.0517 - acc: 0.9182 - val_loss: 1.1120 - val_acc: 0.9258\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 1.0223 - acc: 0.9266 - val_loss: 1.1013 - val_acc: 0.9226\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 1.0110 - acc: 0.9275 - val_loss: 1.0967 - val_acc: 0.9268\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 100s 661ms/step - loss: 0.9915 - acc: 0.9318 - val_loss: 1.0776 - val_acc: 0.9299\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 100s 664ms/step - loss: 0.9852 - acc: 0.9330 - val_loss: 1.0792 - val_acc: 0.9330\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 0.9616 - acc: 0.9389 - val_loss: 1.0356 - val_acc: 0.9316\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.9638 - acc: 0.9379 - val_loss: 1.0621 - val_acc: 0.9363\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 100s 664ms/step - loss: 0.9396 - acc: 0.9433 - val_loss: 1.0651 - val_acc: 0.9370\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.9306 - acc: 0.9456 - val_loss: 1.0582 - val_acc: 0.9347\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.9257 - acc: 0.9486 - val_loss: 1.0264 - val_acc: 0.9355\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 0.9146 - acc: 0.9507 - val_loss: 1.0548 - val_acc: 0.9388\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 100s 664ms/step - loss: 0.9008 - acc: 0.9545 - val_loss: 1.0408 - val_acc: 0.9428\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 101s 668ms/step - loss: 0.8885 - acc: 0.9564 - val_loss: 1.0288 - val_acc: 0.9477\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 103s 684ms/step - loss: 0.8891 - acc: 0.9570 - val_loss: 1.0562 - val_acc: 0.9401\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 0.8632 - acc: 0.9606 - val_loss: 1.0253 - val_acc: 0.9473\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 0.8501 - acc: 0.9656 - val_loss: 1.0144 - val_acc: 0.9488\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 0.8470 - acc: 0.9666 - val_loss: 1.0272 - val_acc: 0.9471\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.8525 - acc: 0.9663 - val_loss: 1.0012 - val_acc: 0.9506\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 100s 664ms/step - loss: 0.8532 - acc: 0.9654 - val_loss: 0.9957 - val_acc: 0.9523\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.8528 - acc: 0.9649 - val_loss: 0.9886 - val_acc: 0.9525\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 0.8530 - acc: 0.9640 - val_loss: 1.0043 - val_acc: 0.9488\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.8450 - acc: 0.9667 - val_loss: 1.0202 - val_acc: 0.9494\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.8467 - acc: 0.9668 - val_loss: 0.9961 - val_acc: 0.9535\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.8422 - acc: 0.9676 - val_loss: 1.0074 - val_acc: 0.9494\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 0.8408 - acc: 0.9694 - val_loss: 0.9992 - val_acc: 0.9519\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 98s 650ms/step - loss: 0.8400 - acc: 0.9681 - val_loss: 1.0000 - val_acc: 0.9517\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 99s 652ms/step - loss: 0.8424 - acc: 0.9675 - val_loss: 1.0073 - val_acc: 0.9513\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.8438 - acc: 0.9666 - val_loss: 0.9997 - val_acc: 0.9523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f240cb68258>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f2495ee7728>\n",
      "RuntimeError: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 2.8398 - acc: 0.7121 - val_loss: 1.9770 - val_acc: 0.7573\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 101s 667ms/step - loss: 1.9002 - acc: 0.7857 - val_loss: 1.6130 - val_acc: 0.8220\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 1.6674 - acc: 0.8167 - val_loss: 1.4493 - val_acc: 0.8502\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 101s 667ms/step - loss: 1.5253 - acc: 0.8373 - val_loss: 1.3748 - val_acc: 0.8629\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 1.4425 - acc: 0.8480 - val_loss: 1.3147 - val_acc: 0.8797\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 1.3669 - acc: 0.8608 - val_loss: 1.2697 - val_acc: 0.8855\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 1.3183 - acc: 0.8687 - val_loss: 1.2137 - val_acc: 0.8961\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 1.2542 - acc: 0.8798 - val_loss: 1.1951 - val_acc: 0.9062\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 1.2291 - acc: 0.8841 - val_loss: 1.1860 - val_acc: 0.9050\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 101s 672ms/step - loss: 1.1935 - acc: 0.8912 - val_loss: 1.1480 - val_acc: 0.9102\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 1.1749 - acc: 0.8943 - val_loss: 1.1352 - val_acc: 0.9098\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 102s 672ms/step - loss: 1.1359 - acc: 0.9034 - val_loss: 1.0795 - val_acc: 0.9193\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 99s 652ms/step - loss: 1.1208 - acc: 0.9052 - val_loss: 1.1096 - val_acc: 0.9191\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 1.0970 - acc: 0.9137 - val_loss: 1.0823 - val_acc: 0.9286\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 1.0858 - acc: 0.9147 - val_loss: 1.0759 - val_acc: 0.9233\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 99s 655ms/step - loss: 1.0513 - acc: 0.9176 - val_loss: 1.0602 - val_acc: 0.9253\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 101s 667ms/step - loss: 1.0320 - acc: 0.9240 - val_loss: 1.0289 - val_acc: 0.9295\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 99s 652ms/step - loss: 1.0221 - acc: 0.9275 - val_loss: 1.0345 - val_acc: 0.9268\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 1.0059 - acc: 0.9299 - val_loss: 1.0211 - val_acc: 0.9303\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 101s 667ms/step - loss: 0.9848 - acc: 0.9331 - val_loss: 1.0184 - val_acc: 0.9324\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 101s 668ms/step - loss: 0.9859 - acc: 0.9330 - val_loss: 0.9944 - val_acc: 0.9398\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 101s 668ms/step - loss: 0.9563 - acc: 0.9405 - val_loss: 1.0002 - val_acc: 0.9407\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.9564 - acc: 0.9420 - val_loss: 1.0175 - val_acc: 0.9386\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.9272 - acc: 0.9462 - val_loss: 0.9782 - val_acc: 0.9394\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.9279 - acc: 0.9476 - val_loss: 0.9679 - val_acc: 0.9386\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 0.9194 - acc: 0.9489 - val_loss: 0.9785 - val_acc: 0.9444\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 99s 655ms/step - loss: 0.9141 - acc: 0.9509 - val_loss: 0.9553 - val_acc: 0.9442\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.9022 - acc: 0.9529 - val_loss: 0.9967 - val_acc: 0.9444\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.8895 - acc: 0.9541 - val_loss: 0.9768 - val_acc: 0.9444\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 0.8771 - acc: 0.9579 - val_loss: 0.9432 - val_acc: 0.9533\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 100s 660ms/step - loss: 0.8747 - acc: 0.9597 - val_loss: 0.9541 - val_acc: 0.9510\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.8779 - acc: 0.9590 - val_loss: 0.9504 - val_acc: 0.9448\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 0.8523 - acc: 0.9636 - val_loss: 0.9629 - val_acc: 0.9504\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 107s 707ms/step - loss: 0.8547 - acc: 0.9637 - val_loss: 0.9884 - val_acc: 0.9533\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 101s 668ms/step - loss: 0.8349 - acc: 0.9689 - val_loss: 0.9123 - val_acc: 0.9589\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 98s 651ms/step - loss: 0.8273 - acc: 0.9711 - val_loss: 0.9240 - val_acc: 0.9575\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 100s 665ms/step - loss: 0.8232 - acc: 0.9725 - val_loss: 0.9066 - val_acc: 0.9610\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 0.8193 - acc: 0.9725 - val_loss: 0.9083 - val_acc: 0.9612\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 98s 652ms/step - loss: 0.8159 - acc: 0.9750 - val_loss: 0.9065 - val_acc: 0.9598\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.8177 - acc: 0.9748 - val_loss: 0.9136 - val_acc: 0.9591\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 99s 656ms/step - loss: 0.8150 - acc: 0.9737 - val_loss: 0.9171 - val_acc: 0.9577\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 101s 667ms/step - loss: 0.8147 - acc: 0.9732 - val_loss: 0.8971 - val_acc: 0.9625\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.8173 - acc: 0.9738 - val_loss: 0.9175 - val_acc: 0.9600\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8172 - acc: 0.9732 - val_loss: 0.9148 - val_acc: 0.9575\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 99s 653ms/step - loss: 0.8184 - acc: 0.9734 - val_loss: 0.9101 - val_acc: 0.9616\n",
      "Epoch 46/200\n",
      "151/151 [==============================] - 100s 661ms/step - loss: 0.8179 - acc: 0.9730 - val_loss: 0.9038 - val_acc: 0.9616\n",
      "Epoch 47/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8170 - acc: 0.9735 - val_loss: 0.9151 - val_acc: 0.9591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f24c75eddb0>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f240cb68258>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 127s 842ms/step - loss: 2.7557 - acc: 0.7165 - val_loss: 1.9497 - val_acc: 0.7789\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 1.8726 - acc: 0.7893 - val_loss: 1.5892 - val_acc: 0.8262\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 1.6535 - acc: 0.8168 - val_loss: 1.4382 - val_acc: 0.8559\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 1.5188 - acc: 0.8356 - val_loss: 1.3440 - val_acc: 0.8717\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 1.4275 - acc: 0.8516 - val_loss: 1.2870 - val_acc: 0.8856\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 1.3448 - acc: 0.8651 - val_loss: 1.2563 - val_acc: 0.8912\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 101s 668ms/step - loss: 1.2982 - acc: 0.8756 - val_loss: 1.2200 - val_acc: 0.8951\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 101s 671ms/step - loss: 1.2382 - acc: 0.8841 - val_loss: 1.1844 - val_acc: 0.9014\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 1.2008 - acc: 0.8913 - val_loss: 1.1668 - val_acc: 0.9093\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 1.1740 - acc: 0.8937 - val_loss: 1.1550 - val_acc: 0.9107\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 101s 671ms/step - loss: 1.1373 - acc: 0.9047 - val_loss: 1.1383 - val_acc: 0.9196\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 1.1164 - acc: 0.9078 - val_loss: 1.1257 - val_acc: 0.9172\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 101s 671ms/step - loss: 1.0938 - acc: 0.9118 - val_loss: 1.1015 - val_acc: 0.9225\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 99s 656ms/step - loss: 1.0836 - acc: 0.9124 - val_loss: 1.1346 - val_acc: 0.9199\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 1.0498 - acc: 0.9188 - val_loss: 1.0821 - val_acc: 0.9252\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 1.0347 - acc: 0.9225 - val_loss: 1.0781 - val_acc: 0.9323\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 1.0111 - acc: 0.9279 - val_loss: 1.0463 - val_acc: 0.9317\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.9952 - acc: 0.9298 - val_loss: 1.0521 - val_acc: 0.9309\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.9854 - acc: 0.9341 - val_loss: 1.0405 - val_acc: 0.9306\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 101s 671ms/step - loss: 0.9630 - acc: 0.9375 - val_loss: 1.0647 - val_acc: 0.9379\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.9513 - acc: 0.9420 - val_loss: 1.0135 - val_acc: 0.9387\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.9428 - acc: 0.9428 - val_loss: 1.0351 - val_acc: 0.9398\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.9244 - acc: 0.9472 - val_loss: 1.0215 - val_acc: 0.9410\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.9207 - acc: 0.9487 - val_loss: 0.9971 - val_acc: 0.9412\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.9067 - acc: 0.9491 - val_loss: 1.0048 - val_acc: 0.9454\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 0.9066 - acc: 0.9517 - val_loss: 0.9918 - val_acc: 0.9466\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.8910 - acc: 0.9552 - val_loss: 0.9818 - val_acc: 0.9487\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8788 - acc: 0.9578 - val_loss: 0.9875 - val_acc: 0.9462\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 99s 656ms/step - loss: 0.8708 - acc: 0.9596 - val_loss: 0.9860 - val_acc: 0.9475\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8579 - acc: 0.9610 - val_loss: 1.0075 - val_acc: 0.9479\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 109s 719ms/step - loss: 0.8640 - acc: 0.9607 - val_loss: 0.9830 - val_acc: 0.9504\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 102s 673ms/step - loss: 0.8420 - acc: 0.9676 - val_loss: 0.9639 - val_acc: 0.9525\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.8297 - acc: 0.9704 - val_loss: 0.9390 - val_acc: 0.9562\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.8393 - acc: 0.9689 - val_loss: 0.9496 - val_acc: 0.9566\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 99s 658ms/step - loss: 0.8249 - acc: 0.9714 - val_loss: 0.9536 - val_acc: 0.9551\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8324 - acc: 0.9698 - val_loss: 0.9574 - val_acc: 0.9560\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 101s 671ms/step - loss: 0.8274 - acc: 0.9710 - val_loss: 0.9482 - val_acc: 0.9581\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 101s 671ms/step - loss: 0.8293 - acc: 0.9712 - val_loss: 0.9445 - val_acc: 0.9587\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8298 - acc: 0.9695 - val_loss: 0.9547 - val_acc: 0.9564\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.8255 - acc: 0.9725 - val_loss: 0.9377 - val_acc: 0.9593\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8267 - acc: 0.9695 - val_loss: 0.9559 - val_acc: 0.9568\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8311 - acc: 0.9686 - val_loss: 0.9363 - val_acc: 0.9578\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8214 - acc: 0.9730 - val_loss: 0.9566 - val_acc: 0.9560\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8239 - acc: 0.9721 - val_loss: 0.9658 - val_acc: 0.9543\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 99s 654ms/step - loss: 0.8257 - acc: 0.9709 - val_loss: 0.9448 - val_acc: 0.9562\n"
     ]
    }
   ],
   "source": [
    "pred = np.zeros((test_s1.shape[0], 17))\n",
    "test_s3 = np.concatenate([test_s1, test_s2], axis=-1)\n",
    "for i ,(tr_ind, val_ind) in enumerate(skf.split(valid_s1, y)):\n",
    "    tr1, tr2, trl = valid_s1[tr_ind], valid_s2[tr_ind], valid_label[tr_ind]\n",
    "    va1, va2, val = valid_s1[val_ind], valid_s2[val_ind], valid_label[val_ind]\n",
    "    save_dir = './model_2/'\n",
    "    model_name = 'LCZ_model_FOLD_3_input_%s_loss-f-74.h5' % str(i)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc',save_best_only=True, save_weights_only=True)\n",
    "    lr_reducr = ReduceLROnPlateau(factor=0.1,cooldown=0, patience=3, min_lr=1e-6)\n",
    "    early = EarlyStopping(monitor='val_acc',patience=5,)\n",
    "    calls = [checkpoint, lr_reducr, early]\n",
    "    class_num = [ 256., 1254., 2353.,  849.,  757., 1906.,  474., 3395., 1914.,\n",
    "        860., 2287.,  382., 1202., 2747.,  202.,  672., 2609.]\n",
    "    class_num = [int(x*0.8) for x in class_num]\n",
    "    model.compile(loss=focal_loss(class_nums), optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "    model.load_weights('./models/LCZ_model_3_inputsss-950-loss-f-74.h5')\n",
    "    tr_gen = data_generate2(tr1, tr2, trl,argumentation=True,batch_size=128,shuffle=True)\n",
    "    va_gen = data_generate2(va1, va2, val,argumentation=True,batch_size=128,shuffle=False)\n",
    "    model.fit_generator(tr_gen, steps_per_epoch=19295//128+1, epochs=200, validation_data=va_gen,validation_steps=4823//128+1, callbacks=calls)\n",
    "    model.load_weights(filepath)\n",
    "    for j in range(4):\n",
    "        te1 = np.rot90(test_s1,k=j,axes=(1,2))\n",
    "        te2 = np.rot90(test_s2,k=j,axes=(1,2))\n",
    "        te3 = np.rot90(test_s3,k=j,axes=(1,2))\n",
    "        pred += model.predict([te1, te2, te3])\n",
    "    te1 = test_s1[:,::-1,:,:]\n",
    "    te2 = test_s2[:,::-1,:,:]\n",
    "    te3 = test_s3[:,::-1,:,:]\n",
    "    pred += model.predict([te1, te2, te3])\n",
    "    te1 = test_s1[:,:,::-1,:]\n",
    "    te2 = test_s2[:,:,::-1,:]\n",
    "    te3 = test_s3[:,:,::-1,:]\n",
    "    pred += model.predict([te1, te2, te3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4842, 17)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pred / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , ..., 1.00000001, 1.        ,\n",
       "       1.00000001])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.zeros((4842, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i ,ind in enumerate(np.argmax(preds,axis=1)):\n",
    "    result[i][ind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.DataFrame(result,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.to_csv('round2_74-result.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(res_92.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
