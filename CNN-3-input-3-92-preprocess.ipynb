{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense,Conv2D, BatchNormalization, Activation,Dropout\n",
    "from keras.layers import AveragePooling2D, Input, Flatten,GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import skimage\n",
    "from keras.models import Model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Preprocessing(x1, x2):\n",
    "    x1_mean,x1_std = np.mean(x1, axis=(1,2,3)),np.std(x1, axis=(1,2,3))\n",
    "    x2_mean,x2_std = np.mean(x2, axis=(1,2,3)),np.std(x2, axis=(1,2,3))\n",
    "    x1 = (x1-x1_mean.reshape(-1,1,1,1))/x1_std.reshape(-1,1,1,1)\n",
    "    x2 = (x2-x2_mean.reshape(-1,1,1,1))/x2_std.reshape(-1,1,1,1)\n",
    "    return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generate(data_path, batch_size, argumentation = True, shuffle=False):\n",
    "    while True:\n",
    "        fid = h5py.File(data_path, 'r')\n",
    "        data_len = fid['sen1'].shape[0]\n",
    "        c = [i for i in range(int(data_len/batch_size) + 1)]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(c)\n",
    "        for i in c:\n",
    "            t = np.random.rand()\n",
    "            try:\n",
    "                y_b = np.array((fid['label'][i * batch_size: (i+1)*batch_size]))\n",
    "                x_b1 = fid['sen1'][i * batch_size: (i+1)*batch_size]\n",
    "                x_b2 = fid['sen2'][i * batch_size: (i+1)*batch_size]\n",
    "                #x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            #x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            #x_b3 = x_b3[:,:,::-1,:]\n",
    "                x_b1, x_b2 = Preprocessing(x_b1, x_b2)\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)\n",
    "            except:\n",
    "                x_b1 = fid['sen1'][i*batch_size:]\n",
    "                x_b2 = fid['sen2'][i*batch_size:]\n",
    "                \n",
    "                y_b = np.array((fid['label'][i * batch_size:]))\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        \n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            #x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            #x_b3 = x_b3[:,:,::-1,:]\n",
    "                x_b1, x_b2 = Preprocessing(x_b1, x_b2)\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                yield ([x_b1, x_b2, x_b3] , y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_v2(input_tensor, depth, num_classes=17):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    #inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=input_tensor,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #y = Flatten()(x)\n",
    "    #outputs = Dense(num_classes,\n",
    "     #               activation='softmax',\n",
    "     #               kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    #model = Model(inputs=inputs, outputs=outputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "def focal_loss(classes_num, gamma=2., alpha=.25, e=0.1):\n",
    "    # classes_num contains sample number of each classes\n",
    "    def focal_loss_fixed(target_tensor, prediction_tensor):\n",
    "        '''\n",
    "        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes\n",
    "        target_tensor is the label tensor, same shape as predcition_tensor\n",
    "        '''\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.python.ops import array_ops\n",
    "        from keras import backend as K\n",
    "\n",
    "        #1# get focal loss with no balanced weight which presented in paper function (4)\n",
    "        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
    "        one_minus_p = array_ops.where(tf.greater(target_tensor,zeros), target_tensor - prediction_tensor, zeros)\n",
    "        FT = -1 * (one_minus_p ** gamma) * tf.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))\n",
    "\n",
    "        #2# get balanced weight alpha\n",
    "        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
    "\n",
    "        total_num = float(sum(classes_num))\n",
    "        classes_w_t1 = [ total_num / ff for ff in classes_num ]\n",
    "        sum_ = sum(classes_w_t1)\n",
    "        classes_w_t2 = [ ff/sum_ for ff in classes_w_t1 ]   #scale\n",
    "        classes_w_tensor = tf.convert_to_tensor(classes_w_t2, dtype=prediction_tensor.dtype)\n",
    "        classes_weight += classes_w_tensor\n",
    "\n",
    "        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)\n",
    "\n",
    "        #3# get balanced focal loss\n",
    "        balanced_fl = alpha * FT\n",
    "        balanced_fl = tf.reduce_sum(balanced_fl)\n",
    "\n",
    "        #4# add other op to prevent overfit\n",
    "        # reference : https://spaces.ac.cn/archives/4493\n",
    "        nb_classes = len(classes_num)\n",
    "        fianal_loss = (1-e) * balanced_fl + e * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)\n",
    "\n",
    "        return fianal_loss\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape1 = (32, 32, 8)\n",
    "input_shape2 = (32, 32, 10)\n",
    "input_shape3 = (32, 32, 18)\n",
    "input_1 = Input(shape=input_shape1)\n",
    "input_2 = Input(shape=input_shape2)\n",
    "input_3 = Input(shape=input_shape3)\n",
    "L1 = BatchNormalization()(input_1)\n",
    "L2 = BatchNormalization()(input_2)\n",
    "L3 = BatchNormalization()(input_3)\n",
    "L1 = resnet_v2(input_tensor=L1, depth=92)\n",
    "L2 = resnet_v2(input_tensor=L2, depth=92)\n",
    "L3 = resnet_v2(input_tensor=L3, depth=92)\n",
    "\n",
    "#L1 = Conv2D(16, kernel_size=3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(L1)\n",
    "#L2 = Conv2D(16, kernel_size=3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(L2)\n",
    "\n",
    "\n",
    "L = keras.layers.concatenate([L1, L2, L3])\n",
    "L = Dense(512, activation='relu')(L)\n",
    "L = BatchNormalization()(L)\n",
    "L = Dropout(0.5)(L)\n",
    "L = Dense(17, activation='softmax', kernel_initializer='he_normal')(L)\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=L)\n",
    "class_nums = [5068, 24431, 31693,  8651, 16493, 35290,  3269, 39326,\n",
    "       13584, 11954, 42902,  9514,  9165, 41377,  2392,  7898,\n",
    "       49359]\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "model.compile(loss=focal_loss(class_nums), optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = './models'\n",
    "model_name = 'LCZ_model_3_inputsss-950-loss-f-92p.h5'\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc',save_best_only=True, save_weights_only=True)\n",
    "lr_reducr = ReduceLROnPlateau(factor=0.1,cooldown=0, patience=3, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor='val_acc',patience=5)\n",
    "calls = [checkpoint, lr_reducr, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = data_generate('./training.h5', batch_size=128, argumentation=True, shuffle=True)\n",
    "val_gen = data_generate('./validation.h5', batch_size=128, argumentation=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2753/2753 [==============================] - 2553s 928ms/step - loss: 4.4713 - acc: 0.6953 - val_loss: 4.9647 - val_acc: 0.5769\n",
      "Epoch 2/200\n",
      "2753/2753 [==============================] - 2412s 876ms/step - loss: 2.2999 - acc: 0.7771 - val_loss: 4.9147 - val_acc: 0.5865\n",
      "Epoch 3/200\n",
      "2753/2753 [==============================] - 2401s 872ms/step - loss: 1.9101 - acc: 0.8056 - val_loss: 4.2664 - val_acc: 0.5788\n",
      "Epoch 4/200\n",
      "2753/2753 [==============================] - 2403s 873ms/step - loss: 1.7175 - acc: 0.8215 - val_loss: 5.1572 - val_acc: 0.5795\n",
      "Epoch 5/200\n",
      "2753/2753 [==============================] - 2416s 878ms/step - loss: 1.5906 - acc: 0.8346 - val_loss: 4.1035 - val_acc: 0.6012\n",
      "Epoch 6/200\n",
      "2753/2753 [==============================] - 2406s 874ms/step - loss: 1.5123 - acc: 0.8429 - val_loss: 4.5185 - val_acc: 0.6129\n",
      "Epoch 7/200\n",
      "2753/2753 [==============================] - 2393s 869ms/step - loss: 1.4460 - acc: 0.8514 - val_loss: 4.8029 - val_acc: 0.6047\n",
      "Epoch 8/200\n",
      "2753/2753 [==============================] - 2396s 870ms/step - loss: 1.3903 - acc: 0.8591 - val_loss: 4.2227 - val_acc: 0.6118\n",
      "Epoch 9/200\n",
      "2753/2753 [==============================] - 2403s 873ms/step - loss: 1.3509 - acc: 0.8635 - val_loss: 4.8246 - val_acc: 0.6326\n",
      "Epoch 10/200\n",
      "2753/2753 [==============================] - 2398s 871ms/step - loss: 1.1630 - acc: 0.8959 - val_loss: 4.4162 - val_acc: 0.6512\n",
      "Epoch 11/200\n",
      "2753/2753 [==============================] - 2404s 873ms/step - loss: 1.1094 - acc: 0.9043 - val_loss: 4.4369 - val_acc: 0.6511\n",
      "Epoch 12/200\n",
      "2753/2753 [==============================] - 2401s 872ms/step - loss: 1.0820 - acc: 0.9086 - val_loss: 4.5103 - val_acc: 0.6470\n",
      "Epoch 13/200\n",
      "2753/2753 [==============================] - 2393s 869ms/step - loss: 1.0530 - acc: 0.9131 - val_loss: 4.6761 - val_acc: 0.6463\n",
      "Epoch 14/200\n",
      "2753/2753 [==============================] - 2400s 872ms/step - loss: 1.0463 - acc: 0.9148 - val_loss: 4.7144 - val_acc: 0.6473\n",
      "Epoch 15/200\n",
      "2753/2753 [==============================] - 2407s 874ms/step - loss: 1.0426 - acc: 0.9150 - val_loss: 4.7687 - val_acc: 0.6451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42e3f8cd30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, steps_per_epoch=352366//128+1, epochs=200,  verbose=1,callbacks=calls, validation_data=val_gen,\n",
    "                   validation_steps=24119//128+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generate2(data1, data2, data3, batch_size, argumentation = True, shuffle=False):\n",
    "    while True:\n",
    "        data_len = data1.shape[0]\n",
    "        c = [i for i in range(int(data_len/batch_size) + 1)]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(c)\n",
    "        for i in c:\n",
    "            t = np.random.rand()\n",
    "            try:\n",
    "                y_b = data3[i*batch_size:(i+1)*batch_size]\n",
    "                x_b1 = data1[i * batch_size: (i+1)*batch_size]\n",
    "                x_b2 = data2[i * batch_size: (i+1)*batch_size]\n",
    "                \n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            #x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            #x_b3 = x_b3[:,:,::-1,:]\n",
    "                x_b1, x_b2 = Preprocessing(x_b1, x_b2)\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)\n",
    "            except:\n",
    "                y_b = data3[i*batch_size:]\n",
    "                x_b1 = data1[i*batch_size:]\n",
    "                x_b2 = data2[i*batch_size:]\n",
    "                #x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                if argumentation:\n",
    "                    if t < 0.6:\n",
    "                        k = np.random.choice([2,3,4,5,6])\n",
    "                        if k == 2:\n",
    "                            x_b1 = np.rot90(x_b1, k=1, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=1, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=1, axes=(1,2))\n",
    "                        elif k == 3:\n",
    "                            x_b1 = np.rot90(x_b1, k=2, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=2, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=2, axes=(1,2))\n",
    "                        elif k == 4:\n",
    "                            x_b1 = np.rot90(x_b1, k=3, axes=(1,2))\n",
    "                            x_b2 = np.rot90(x_b2, k=3, axes=(1,2))\n",
    "                            #x_b3 = np.rot90(x_b3, k=3, axes=(1,2))\n",
    "                        elif k == 5:\n",
    "                            x_b1 = x_b1[:,::-1,:,:]\n",
    "                            x_b2 = x_b2[:,::-1,:,:]\n",
    "                            #x_b3 = x_b3[:,::-1,:,:]\n",
    "                        elif k == 6:\n",
    "                            x_b1 = x_b1[:,:,::-1,:]\n",
    "                            x_b2 = x_b2[:,:,::-1,:]\n",
    "                            #x_b3 = x_b3[:,:,::-1,:]\n",
    "                x_b1, x_b2 = Preprocessing(x_b1, x_b2)\n",
    "                x_b3 = np.concatenate([x_b1, x_b2], axis=-1)\n",
    "                yield ([x_b1, x_b2, x_b3], y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid = h5py.File('./validation.h5', 'r')\n",
    "valid_s1 = valid['sen1']\n",
    "valid_s2 = valid['sen2']\n",
    "valid_label = valid['label']\n",
    "valid_s1 = np.array(valid_s1)\n",
    "valid_s2 = np.array(valid_s2)\n",
    "valid_label = np.array(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24119, 32, 32, 8), (24119, 32, 32, 10), (24119, 17))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_s1.shape,valid_s2.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018)\n",
    "kf = KFold(n_splits=5, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.argmax(valid_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = h5py.File('./round2_test_b_20190211.h5')\n",
    "test_s1 = np.array(test['sen1'])\n",
    "test_s2 = np.array(test['sen2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19295.2, 4823.8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24119*0.8,24119*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 242s 2s/step - loss: 2.2930 - acc: 0.7578 - val_loss: 1.7508 - val_acc: 0.8189\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 128s 846ms/step - loss: 1.6879 - acc: 0.8188 - val_loss: 1.4696 - val_acc: 0.8561\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 127s 844ms/step - loss: 1.5103 - acc: 0.8407 - val_loss: 1.3476 - val_acc: 0.8640\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 128s 848ms/step - loss: 1.3970 - acc: 0.8557 - val_loss: 1.2772 - val_acc: 0.8853\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 1.3152 - acc: 0.8711 - val_loss: 1.2205 - val_acc: 0.8841\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 128s 845ms/step - loss: 1.2529 - acc: 0.8811 - val_loss: 1.2358 - val_acc: 0.8864\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 128s 848ms/step - loss: 1.2098 - acc: 0.8886 - val_loss: 1.1802 - val_acc: 0.8957\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 128s 848ms/step - loss: 1.1699 - acc: 0.8933 - val_loss: 1.1328 - val_acc: 0.9184\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 1.1229 - acc: 0.9070 - val_loss: 1.1210 - val_acc: 0.9126\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 1.1060 - acc: 0.9092 - val_loss: 1.1026 - val_acc: 0.9141\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 129s 851ms/step - loss: 1.0674 - acc: 0.9136 - val_loss: 1.1043 - val_acc: 0.9276\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 126s 834ms/step - loss: 1.0596 - acc: 0.9167 - val_loss: 1.0876 - val_acc: 0.9182\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 126s 834ms/step - loss: 1.0342 - acc: 0.9234 - val_loss: 1.1146 - val_acc: 0.9255\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 128s 849ms/step - loss: 1.0084 - acc: 0.9287 - val_loss: 1.1034 - val_acc: 0.9311\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 128s 846ms/step - loss: 0.9916 - acc: 0.9328 - val_loss: 1.0895 - val_acc: 0.9292\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 128s 845ms/step - loss: 0.9634 - acc: 0.9379 - val_loss: 1.0488 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 126s 834ms/step - loss: 0.9447 - acc: 0.9417 - val_loss: 1.0423 - val_acc: 0.9348\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 129s 855ms/step - loss: 0.9407 - acc: 0.9453 - val_loss: 1.0184 - val_acc: 0.9373\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 128s 847ms/step - loss: 0.9272 - acc: 0.9464 - val_loss: 1.0378 - val_acc: 0.9408\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 128s 845ms/step - loss: 0.9248 - acc: 0.9495 - val_loss: 1.0272 - val_acc: 0.9422\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 126s 834ms/step - loss: 0.9096 - acc: 0.9510 - val_loss: 1.0165 - val_acc: 0.9420\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 0.9015 - acc: 0.9525 - val_loss: 1.0143 - val_acc: 0.9404\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 128s 845ms/step - loss: 0.8978 - acc: 0.9561 - val_loss: 1.0036 - val_acc: 0.9422\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 128s 847ms/step - loss: 0.8881 - acc: 0.9539 - val_loss: 1.0264 - val_acc: 0.9451\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 0.8959 - acc: 0.9547 - val_loss: 0.9986 - val_acc: 0.9449\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 0.8859 - acc: 0.9557 - val_loss: 1.0181 - val_acc: 0.9449\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 126s 833ms/step - loss: 0.8724 - acc: 0.9592 - val_loss: 1.0029 - val_acc: 0.9449\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 126s 837ms/step - loss: 0.8749 - acc: 0.9590 - val_loss: 0.9994 - val_acc: 0.9437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f51f823c360>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f5235871af0>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 142s 939ms/step - loss: 2.3532 - acc: 0.7534 - val_loss: 1.6718 - val_acc: 0.8095\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 1.7004 - acc: 0.8169 - val_loss: 1.4470 - val_acc: 0.8379\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 1.5181 - acc: 0.8382 - val_loss: 1.3370 - val_acc: 0.8548\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 1.4061 - acc: 0.8542 - val_loss: 1.2952 - val_acc: 0.8528\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 1.3276 - acc: 0.8655 - val_loss: 1.2555 - val_acc: 0.8805\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 1.2798 - acc: 0.8751 - val_loss: 1.1807 - val_acc: 0.8847\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.2118 - acc: 0.8850 - val_loss: 1.1605 - val_acc: 0.8927\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 123s 817ms/step - loss: 1.1734 - acc: 0.8967 - val_loss: 1.1346 - val_acc: 0.8981\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 1.1383 - acc: 0.9029 - val_loss: 1.1545 - val_acc: 0.8826\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 123s 814ms/step - loss: 1.1132 - acc: 0.9053 - val_loss: 1.1108 - val_acc: 0.9047\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 1.0823 - acc: 0.9129 - val_loss: 1.0680 - val_acc: 0.9143\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 123s 817ms/step - loss: 1.0678 - acc: 0.9179 - val_loss: 1.0588 - val_acc: 0.9149\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 124s 819ms/step - loss: 1.0385 - acc: 0.9218 - val_loss: 1.0518 - val_acc: 0.9201\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 1.0263 - acc: 0.9241 - val_loss: 1.0542 - val_acc: 0.9176\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 123s 817ms/step - loss: 1.0046 - acc: 0.9285 - val_loss: 1.0314 - val_acc: 0.9246\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 123s 814ms/step - loss: 0.9918 - acc: 0.9318 - val_loss: 1.0580 - val_acc: 0.9292\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.9669 - acc: 0.9358 - val_loss: 1.0248 - val_acc: 0.9252\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.9594 - acc: 0.9404 - val_loss: 1.0063 - val_acc: 0.9292\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 123s 815ms/step - loss: 0.9446 - acc: 0.9446 - val_loss: 1.0063 - val_acc: 0.9323\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.9350 - acc: 0.9465 - val_loss: 1.0129 - val_acc: 0.9300\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 123s 814ms/step - loss: 0.9276 - acc: 0.9440 - val_loss: 0.9990 - val_acc: 0.9420\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.9147 - acc: 0.9485 - val_loss: 1.0043 - val_acc: 0.9333\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 0.8971 - acc: 0.9551 - val_loss: 0.9632 - val_acc: 0.9449\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.8906 - acc: 0.9547 - val_loss: 0.9955 - val_acc: 0.9366\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.8750 - acc: 0.9591 - val_loss: 0.9728 - val_acc: 0.9404\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 126s 836ms/step - loss: 0.8645 - acc: 0.9622 - val_loss: 0.9718 - val_acc: 0.9505\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 123s 816ms/step - loss: 0.8448 - acc: 0.9651 - val_loss: 0.9259 - val_acc: 0.9536\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.8397 - acc: 0.9679 - val_loss: 0.9350 - val_acc: 0.9449\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.8392 - acc: 0.9670 - val_loss: 0.9598 - val_acc: 0.9468\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.8318 - acc: 0.9685 - val_loss: 0.9299 - val_acc: 0.9513\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 123s 814ms/step - loss: 0.8206 - acc: 0.9713 - val_loss: 0.9149 - val_acc: 0.9551\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.8155 - acc: 0.9746 - val_loss: 0.9406 - val_acc: 0.9538\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 121s 801ms/step - loss: 0.8115 - acc: 0.9753 - val_loss: 0.9199 - val_acc: 0.9536\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.8118 - acc: 0.9743 - val_loss: 0.9306 - val_acc: 0.9520\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 121s 800ms/step - loss: 0.8135 - acc: 0.9749 - val_loss: 0.9045 - val_acc: 0.9520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f52358ae200>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f51f823c360>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 146s 966ms/step - loss: 2.2883 - acc: 0.7549 - val_loss: 1.7053 - val_acc: 0.8181\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 124s 818ms/step - loss: 1.6830 - acc: 0.8131 - val_loss: 1.4541 - val_acc: 0.8557\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 1.4955 - acc: 0.8428 - val_loss: 1.3499 - val_acc: 0.8617\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 124s 819ms/step - loss: 1.3797 - acc: 0.8561 - val_loss: 1.2718 - val_acc: 0.8837\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 1.3115 - acc: 0.8687 - val_loss: 1.2266 - val_acc: 0.8901\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 124s 819ms/step - loss: 1.2564 - acc: 0.8783 - val_loss: 1.2040 - val_acc: 0.8963\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 122s 809ms/step - loss: 1.2093 - acc: 0.8865 - val_loss: 1.1784 - val_acc: 0.8946\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 1.1603 - acc: 0.8986 - val_loss: 1.1645 - val_acc: 0.9046\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 1.1239 - acc: 0.9056 - val_loss: 1.1371 - val_acc: 0.9054\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 124s 821ms/step - loss: 1.0890 - acc: 0.9124 - val_loss: 1.1272 - val_acc: 0.9106\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 124s 819ms/step - loss: 1.0633 - acc: 0.9168 - val_loss: 1.1093 - val_acc: 0.9156\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 124s 823ms/step - loss: 1.0417 - acc: 0.9234 - val_loss: 1.0847 - val_acc: 0.9160\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 124s 819ms/step - loss: 1.0178 - acc: 0.9273 - val_loss: 1.0883 - val_acc: 0.9204\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 1.0078 - acc: 0.9290 - val_loss: 1.0495 - val_acc: 0.9287\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 122s 805ms/step - loss: 0.9983 - acc: 0.9307 - val_loss: 1.0476 - val_acc: 0.9280\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 122s 805ms/step - loss: 0.9905 - acc: 0.9330 - val_loss: 1.0561 - val_acc: 0.9278\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 125s 827ms/step - loss: 0.9519 - acc: 0.9424 - val_loss: 1.0494 - val_acc: 0.9311\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 0.9467 - acc: 0.9436 - val_loss: 1.0273 - val_acc: 0.9353\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 121s 804ms/step - loss: 0.9257 - acc: 0.9474 - val_loss: 1.0425 - val_acc: 0.9341\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 124s 821ms/step - loss: 0.9143 - acc: 0.9491 - val_loss: 1.0641 - val_acc: 0.9357\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 0.9071 - acc: 0.9520 - val_loss: 1.0221 - val_acc: 0.9378\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 122s 805ms/step - loss: 0.8924 - acc: 0.9561 - val_loss: 1.0252 - val_acc: 0.9372\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 0.8915 - acc: 0.9569 - val_loss: 1.0358 - val_acc: 0.9438\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 126s 836ms/step - loss: 0.8766 - acc: 0.9576 - val_loss: 1.0299 - val_acc: 0.9357\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 124s 819ms/step - loss: 0.8534 - acc: 0.9645 - val_loss: 0.9810 - val_acc: 0.9475\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 124s 820ms/step - loss: 0.8538 - acc: 0.9646 - val_loss: 0.9877 - val_acc: 0.9488\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 122s 805ms/step - loss: 0.8394 - acc: 0.9685 - val_loss: 0.9695 - val_acc: 0.9484\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8360 - acc: 0.9690 - val_loss: 0.9676 - val_acc: 0.9486\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 121s 804ms/step - loss: 0.8347 - acc: 0.9700 - val_loss: 0.9945 - val_acc: 0.9484\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 121s 804ms/step - loss: 0.8205 - acc: 0.9729 - val_loss: 0.9761 - val_acc: 0.9479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f4f7e238678>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f52358ae200>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 151s 1s/step - loss: 2.3229 - acc: 0.7538 - val_loss: 1.7222 - val_acc: 0.8108\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 1.7079 - acc: 0.8137 - val_loss: 1.4395 - val_acc: 0.8486\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 124s 823ms/step - loss: 1.5183 - acc: 0.8369 - val_loss: 1.3332 - val_acc: 0.8637\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 125s 827ms/step - loss: 1.3953 - acc: 0.8543 - val_loss: 1.2578 - val_acc: 0.8809\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 1.3169 - acc: 0.8690 - val_loss: 1.2558 - val_acc: 0.8728\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 124s 823ms/step - loss: 1.2804 - acc: 0.8781 - val_loss: 1.2132 - val_acc: 0.8849\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 124s 821ms/step - loss: 1.2125 - acc: 0.8876 - val_loss: 1.2091 - val_acc: 0.8959\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 124s 821ms/step - loss: 1.1705 - acc: 0.8945 - val_loss: 1.1345 - val_acc: 0.9108\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 1.1452 - acc: 0.9005 - val_loss: 1.1498 - val_acc: 0.9060\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 125s 826ms/step - loss: 1.1133 - acc: 0.9059 - val_loss: 1.1222 - val_acc: 0.9218\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 1.0761 - acc: 0.9131 - val_loss: 1.1154 - val_acc: 0.9191\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 1.0557 - acc: 0.9180 - val_loss: 1.0903 - val_acc: 0.9203\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 124s 821ms/step - loss: 1.0324 - acc: 0.9202 - val_loss: 1.0519 - val_acc: 0.9276\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 121s 804ms/step - loss: 1.0088 - acc: 0.9266 - val_loss: 1.0826 - val_acc: 0.9222\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.9933 - acc: 0.9312 - val_loss: 1.0813 - val_acc: 0.9253\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 130s 863ms/step - loss: 0.9745 - acc: 0.9358 - val_loss: 1.0854 - val_acc: 0.9313\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 0.9498 - acc: 0.9423 - val_loss: 1.0216 - val_acc: 0.9355\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.9405 - acc: 0.9449 - val_loss: 1.0157 - val_acc: 0.9330\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 0.9306 - acc: 0.9467 - val_loss: 1.0352 - val_acc: 0.9392\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 0.9163 - acc: 0.9493 - val_loss: 1.0348 - val_acc: 0.9432\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.9078 - acc: 0.9506 - val_loss: 1.0150 - val_acc: 0.9374\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 0.8982 - acc: 0.9531 - val_loss: 1.0161 - val_acc: 0.9448\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8944 - acc: 0.9552 - val_loss: 1.0225 - val_acc: 0.9434\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8967 - acc: 0.9531 - val_loss: 1.0302 - val_acc: 0.9415\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 124s 821ms/step - loss: 0.8839 - acc: 0.9562 - val_loss: 1.0056 - val_acc: 0.9463\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 0.8728 - acc: 0.9608 - val_loss: 1.0189 - val_acc: 0.9465\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8762 - acc: 0.9592 - val_loss: 0.9889 - val_acc: 0.9446\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8662 - acc: 0.9607 - val_loss: 1.0101 - val_acc: 0.9448\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 124s 821ms/step - loss: 0.8640 - acc: 0.9616 - val_loss: 1.0003 - val_acc: 0.9471\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8580 - acc: 0.9642 - val_loss: 0.9993 - val_acc: 0.9467\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 0.8566 - acc: 0.9629 - val_loss: 1.0092 - val_acc: 0.9486\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 121s 805ms/step - loss: 0.8500 - acc: 0.9666 - val_loss: 0.9840 - val_acc: 0.9477\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8528 - acc: 0.9647 - val_loss: 0.9923 - val_acc: 0.9479\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 0.8418 - acc: 0.9689 - val_loss: 0.9875 - val_acc: 0.9540\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 122s 807ms/step - loss: 0.8459 - acc: 0.9677 - val_loss: 0.9787 - val_acc: 0.9517\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 121s 804ms/step - loss: 0.8482 - acc: 0.9669 - val_loss: 1.0100 - val_acc: 0.9486\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8451 - acc: 0.9653 - val_loss: 0.9872 - val_acc: 0.9506\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 121s 804ms/step - loss: 0.8452 - acc: 0.9684 - val_loss: 1.0032 - val_acc: 0.9513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object data_generate2 at 0x7f4ff1e65f10>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "Exception ignored in: <generator object data_generate2 at 0x7f4f7e238678>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 154s 1s/step - loss: 2.3046 - acc: 0.7586 - val_loss: 1.6595 - val_acc: 0.8167\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 124s 824ms/step - loss: 1.6583 - acc: 0.8205 - val_loss: 1.4492 - val_acc: 0.8455\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 1.4750 - acc: 0.8439 - val_loss: 1.3400 - val_acc: 0.8553\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 1.3692 - acc: 0.8591 - val_loss: 1.2913 - val_acc: 0.8717\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 125s 825ms/step - loss: 1.3136 - acc: 0.8703 - val_loss: 1.2329 - val_acc: 0.8850\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 124s 824ms/step - loss: 1.2485 - acc: 0.8836 - val_loss: 1.2203 - val_acc: 0.8891\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 124s 824ms/step - loss: 1.1934 - acc: 0.8917 - val_loss: 1.2203 - val_acc: 0.9049\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 121s 802ms/step - loss: 1.1466 - acc: 0.9022 - val_loss: 1.1597 - val_acc: 0.9003\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 124s 823ms/step - loss: 1.1248 - acc: 0.9056 - val_loss: 1.1509 - val_acc: 0.9068\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 124s 824ms/step - loss: 1.0888 - acc: 0.9117 - val_loss: 1.1181 - val_acc: 0.9128\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 122s 805ms/step - loss: 1.0581 - acc: 0.9175 - val_loss: 1.1569 - val_acc: 0.9088\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 124s 824ms/step - loss: 1.0449 - acc: 0.9218 - val_loss: 1.0830 - val_acc: 0.9207\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 1.0252 - acc: 0.9236 - val_loss: 1.0937 - val_acc: 0.9167\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 124s 823ms/step - loss: 1.0092 - acc: 0.9292 - val_loss: 1.0792 - val_acc: 0.9311\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 121s 804ms/step - loss: 0.9891 - acc: 0.9314 - val_loss: 1.0651 - val_acc: 0.9271\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.9655 - acc: 0.9378 - val_loss: 1.0813 - val_acc: 0.9263\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 125s 825ms/step - loss: 0.9520 - acc: 0.9400 - val_loss: 1.0515 - val_acc: 0.9336\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 125s 829ms/step - loss: 0.9526 - acc: 0.9393 - val_loss: 1.0608 - val_acc: 0.9354\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.9289 - acc: 0.9466 - val_loss: 1.0166 - val_acc: 0.9323\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 124s 823ms/step - loss: 0.9220 - acc: 0.9473 - val_loss: 1.0391 - val_acc: 0.9375\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 122s 805ms/step - loss: 0.9036 - acc: 0.9528 - val_loss: 1.0600 - val_acc: 0.9371\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 129s 855ms/step - loss: 0.8933 - acc: 0.9544 - val_loss: 1.0815 - val_acc: 0.9356\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 125s 826ms/step - loss: 0.8800 - acc: 0.9578 - val_loss: 1.0439 - val_acc: 0.9394\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 124s 824ms/step - loss: 0.8644 - acc: 0.9625 - val_loss: 1.0061 - val_acc: 0.9406\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 124s 823ms/step - loss: 0.8552 - acc: 0.9638 - val_loss: 0.9975 - val_acc: 0.9437\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 122s 805ms/step - loss: 0.8523 - acc: 0.9644 - val_loss: 0.9970 - val_acc: 0.9419\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 0.8522 - acc: 0.9648 - val_loss: 0.9886 - val_acc: 0.9468\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 125s 825ms/step - loss: 0.8400 - acc: 0.9664 - val_loss: 1.0088 - val_acc: 0.9489\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 121s 802ms/step - loss: 0.8452 - acc: 0.9649 - val_loss: 0.9999 - val_acc: 0.9462\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 121s 802ms/step - loss: 0.8344 - acc: 0.9683 - val_loss: 0.9857 - val_acc: 0.9460\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 124s 822ms/step - loss: 0.8270 - acc: 0.9694 - val_loss: 0.9721 - val_acc: 0.9502\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8239 - acc: 0.9704 - val_loss: 0.9909 - val_acc: 0.9489\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 124s 824ms/step - loss: 0.8104 - acc: 0.9747 - val_loss: 0.9889 - val_acc: 0.9549\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 121s 802ms/step - loss: 0.8192 - acc: 0.9713 - val_loss: 0.9724 - val_acc: 0.9541\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8045 - acc: 0.9740 - val_loss: 0.9678 - val_acc: 0.9547\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 125s 825ms/step - loss: 0.7987 - acc: 0.9765 - val_loss: 0.9574 - val_acc: 0.9560\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.7995 - acc: 0.9768 - val_loss: 0.9746 - val_acc: 0.9510\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.8018 - acc: 0.9760 - val_loss: 0.9702 - val_acc: 0.9535\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.7937 - acc: 0.9785 - val_loss: 0.9621 - val_acc: 0.9554\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 121s 803ms/step - loss: 0.7864 - acc: 0.9804 - val_loss: 0.9681 - val_acc: 0.9558\n"
     ]
    }
   ],
   "source": [
    "pred = np.zeros((test_s1.shape[0], 17))\n",
    "test_s3 = np.concatenate([test_s1, test_s2], axis=-1)\n",
    "for i ,(tr_ind, val_ind) in enumerate(skf.split(valid_s1, y)):\n",
    "    tr1, tr2, trl = valid_s1[tr_ind], valid_s2[tr_ind], valid_label[tr_ind]\n",
    "    va1, va2, val = valid_s1[val_ind], valid_s2[val_ind], valid_label[val_ind]\n",
    "    save_dir = './model_2/'\n",
    "    model_name = 'LCZ_model_FOLD_3_input_%s_loss-f-92p.h5' % str(i)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc',save_best_only=True, save_weights_only=True)\n",
    "    lr_reducr = ReduceLROnPlateau(factor=0.5,cooldown=0, patience=2, min_lr=1e-6)\n",
    "    early = EarlyStopping(monitor='val_acc',patience=4,)\n",
    "    calls = [checkpoint, lr_reducr, early]\n",
    "    class_num = [ 256., 1254., 2353.,  849.,  757., 1906.,  474., 3395., 1914.,\n",
    "        860., 2287.,  382., 1202., 2747.,  202.,  672., 2609.]\n",
    "    class_num = [int(x*0.8) for x in class_num]\n",
    "    model.compile(loss=focal_loss(class_nums), optimizer=Adam(lr=0.0002), metrics=['accuracy'])\n",
    "    model.load_weights('./models/LCZ_model_3_inputsss-950-loss-f-92p.h5')\n",
    "    tr_gen = data_generate2(tr1, tr2, trl,argumentation=True,batch_size=128,shuffle=True)\n",
    "    va_gen = data_generate2(va1, va2, val,argumentation=True,batch_size=128,shuffle=False)\n",
    "    model.fit_generator(tr_gen, steps_per_epoch=19295//128+1, epochs=200, validation_data=va_gen,validation_steps=4823//128+1, callbacks=calls)\n",
    "    model.load_weights(filepath)\n",
    "    for j in range(4):\n",
    "        te1 = np.rot90(test_s1,k=j,axes=(1,2))\n",
    "        te2 = np.rot90(test_s2,k=j,axes=(1,2))\n",
    "        te1, te2 = Preprocessing(te1, te2)\n",
    "        te3 = np.concatenate([te1, te2], axis=-1)\n",
    "        #te3 = np.rot90(test_s3,k=i,axes=(1,2))\n",
    "        pred += model.predict([te1, te2, te3])\n",
    "    te1 = test_s1[:,::-1,:,:]\n",
    "    te2 = test_s2[:,::-1,:,:]\n",
    "    te1, te2 = Preprocessing(te1, te2)\n",
    "    te3 = np.concatenate([te1, te2], axis=-1)\n",
    "    pred += model.predict([te1, te2, te3])\n",
    "    te1 = test_s1[:,:,::-1,:]\n",
    "    te2 = test_s2[:,:,::-1,:]\n",
    "    te1, te2 = Preprocessing(te1, te2)\n",
    "    te3 = np.concatenate([te1, te2], axis=-1)\n",
    "    pred += model.predict([te1, te2, te3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4842, 17)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pred / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , ..., 1.00000001, 1.        ,\n",
       "       1.00000001])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.zeros((4842, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i ,ind in enumerate(np.argmax(preds,axis=1)):\n",
    "    result[i][ind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.DataFrame(result,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.to_csv('round2_74-result.csv', header=None, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
